{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/shakespeare/input.txt'\n",
    "with open(data_path, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\", 65)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)\n",
    "''.join(vocab), vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {c: i for i, c in enumerate(vocab)}\n",
    "itos = {i: c for i, c in enumerate(vocab)}\n",
    "\n",
    "def encode(s):\n",
    "    return torch.tensor([stoi[c] for c in s], dtype=torch.long)\n",
    "\n",
    "def decode(t):\n",
    "    return ''.join(itos[i] for i in t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([46, 43, 50, 50, 53,  1, 61, 53, 56, 50, 42]), 'hello world')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('hello world'), decode(encode('hello world').tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = encode(text)\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115394, 892315, 223079)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 0.8\n",
    "train_data = data[:int(len(data)*split)]\n",
    "val_data = data[int(len(data)*split):]\n",
    "len(data), len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following tensor tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]) contains following 8 datapoints: \n",
      "Sequence tensor([18]) should be followed by 47\n",
      "Sequence tensor([18, 47]) should be followed by 56\n",
      "Sequence tensor([18, 47, 56]) should be followed by 57\n",
      "Sequence tensor([18, 47, 56, 57]) should be followed by 58\n",
      "Sequence tensor([18, 47, 56, 57, 58]) should be followed by 1\n",
      "Sequence tensor([18, 47, 56, 57, 58,  1]) should be followed by 15\n",
      "Sequence tensor([18, 47, 56, 57, 58,  1, 15]) should be followed by 47\n",
      "Sequence tensor([18, 47, 56, 57, 58,  1, 15, 47]) should be followed by 58\n"
     ]
    }
   ],
   "source": [
    "def tmp():\n",
    "    tmp = data[:sequence_length+1]\n",
    "    print(f'The following tensor {tmp} contains following {sequence_length} datapoints: ')\n",
    "    for i in range(1,sequence_length+1):\n",
    "        print(f'Sequence {tmp[:i]} should be followed by {tmp[i]}')\n",
    "\n",
    "tmp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[47, 57, 10,  1, 39, 52, 42,  1],\n",
       "         [59, 56,  1, 46, 43, 39, 56, 58],\n",
       "         [32, 46, 39, 58,  1, 39, 50, 61],\n",
       "         [26, 53, 58, 46, 47, 52, 45,  1]]),\n",
       " tensor([[57, 10,  1, 39, 52, 42,  1, 50],\n",
       "         [56,  1, 46, 43, 39, 56, 58, 57],\n",
       "         [46, 39, 58,  1, 39, 50, 61, 39],\n",
       "         [53, 58, 46, 47, 52, 45,  1, 40]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "def get_batches(data):\n",
    "    idx = torch.randint(len(data) - sequence_length, (batch_size, ))\n",
    "    xb = torch.stack([data[i:i+sequence_length] for i in idx], dim=0)\n",
    "    yb = torch.stack([data[i+1:i+sequence_length+1] for i in idx], dim=0)\n",
    "    return xb, yb\n",
    "\n",
    "x, y = get_batches(train_data)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, ixs, targets=None):\n",
    "        # ixs: (b,t)\n",
    "        # targets: (b,t)\n",
    "        logits = self.embed(ixs) # (b,t,c=vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.permute(0,2,1) # (b,c,t)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            logits = logits.permute(0,2,1) # back to (b,t,c)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, ixs, max_len):\n",
    "        \"\"\"\n",
    "        ixs: (b,t) - input sequence to start generating from\n",
    "        max_len: int - maximum length of the generated sequence\n",
    "        \"\"\"\n",
    "        b, t = ixs.shape\n",
    "        for _ in range(max_len):\n",
    "            # generation (b, ) next tokens in parallel\n",
    "            logits, loss = self.forward(ixs) # logits=(b,t,c), loss is ignored\n",
    "            # get juse the final timestep\n",
    "            last_logits = logits[:, -1, :] # (b,c)\n",
    "            # normalize\n",
    "            last_probs = F.softmax(last_logits, dim=-1) # across c\n",
    "            next_tokens = torch.multinomial(last_probs, 1) # (b,c) -> (b)\n",
    "            ixs = torch.cat((ixs, next_tokens), dim=1) # across t so (b,t) -> (b, t+1)\n",
    "        return ixs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmp_ce():\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    x = torch.randn((2,3))\n",
    "    sx = F.softmax(x, dim=-1)\n",
    "    y = torch.randint(3, (2, ), dtype=torch.long)\n",
    "    loss = loss_fn(x, y)\n",
    "    print('x==')\n",
    "    print(x)\n",
    "    print('sx==')\n",
    "    print(sx)\n",
    "    print('y==')\n",
    "    print(y)\n",
    "    print('loss==')\n",
    "    print(loss)\n",
    "    # x = torch.randn(size=(4,8,65)) # (b,t,c)\n",
    "    # y = torch.randn(size=(4,8)) # (b,t)\n",
    "# tmp_ce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmp_multinomial():\n",
    "    x = torch.tensor([0.1, 0.3, 0.9])\n",
    "    print(torch.multinomial(x, num_samples=3))\n",
    "# tmp_multinomial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "blm = BigramLanguageModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cfYCDRUZsYBsA?Y?vgB!ZWOEiAoezL:q&Avufr?gSGdWrp&Bxt-R?wo'TYhBChdIC-RDaRmEGENyouVg'UjyQNyQSpZUVeN:BZqh\n"
     ]
    }
   ],
   "source": [
    "start_ix = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(blm.generate(start_ix, 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimzer = torch.optim.AdamW(blm.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4050023555755615\n",
      "\n",
      "TEpipe.\n",
      "Ditharomy rells y Bef podst,\n",
      "Ath m tha otarizewice'ds aghadeat songgritindiuse, maisfeadorre\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_epochs=10000\n",
    "for i in range(num_epochs):\n",
    "    xb, yb = get_batches(train_data)\n",
    "    logits, loss = blm(xb, yb)\n",
    "\n",
    "    # print(f'Epoch {i}: Loss={loss.item()}')\n",
    "\n",
    "    optimzer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimzer.step()\n",
    "\n",
    "print(loss.item())\n",
    "print(decode(blm.generate(start_ix, 100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0653, 0.9347, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1879, 0.0124, 0.7998, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0656, 0.5746, 0.2039, 0.1560, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1196, 0.1878, 0.6265, 0.0085, 0.0576, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1301, 0.0961, 0.4267, 0.0108, 0.1413, 0.1950, 0.0000, 0.0000],\n",
       "        [0.0171, 0.5209, 0.0979, 0.0920, 0.0499, 0.1279, 0.0944, 0.0000],\n",
       "        [0.0840, 0.1627, 0.3036, 0.0377, 0.0459, 0.3308, 0.0166, 0.0188]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,32\n",
    "x = torch.randn((B,T,C))\n",
    "head_size=16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B,T,C) -> (B,T,head_size)\n",
    "q = query(x) # (B,T,C) -> (B,T,head_size)\n",
    "v = value(x)\n",
    "\n",
    "wei = k @ q.transpose(-2,-1) # (B,T,head_size) @ (B,head_size,T) -> (B,T,T)\n",
    "\n",
    "tril = torch.tril(torch.ones((8,8)))\n",
    "# wei = torch.zeros((8,8))\n",
    "wei = wei.masked_fill((tril == 0.), -torch.inf)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow = wei @ v # (b,t,c)\n",
    "xbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m     wei \u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m@\u001b[39m q\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;66;03m# (24 ** (-0.5))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(wei\u001b[38;5;241m.\u001b[39mvar())\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtmp_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m, in \u001b[0;36mtmp_var\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtmp_var\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m24\u001b[39m))\n\u001b[1;32m      3\u001b[0m     k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m24\u001b[39m))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(k\u001b[38;5;241m.\u001b[39mvar())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def tmp_var():\n",
    "    q = torch.randn((8,24))\n",
    "    k = torch.randn((8,24))\n",
    "    print(k.var())\n",
    "    print(v.var())\n",
    "    wei = k @ q.T  # (24 ** (-0.5))\n",
    "    print(wei.var())\n",
    "\n",
    "\n",
    "tmp_var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and using datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssg2/miniconda3/envs/idl/lib/python3.8/site-packages/datasets/load.py:1461: FutureWarning: The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d433e3389ae3425986118f41ab912c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Skylion007/openwebtext\", split='train') # only has one split - train\n",
    "dataset = dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15339, 1917]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "enc.encode('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 8013769\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80137"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBSET_PERCENTAGE=0.01 # 1%, between 0 and 1\n",
    "\n",
    "# data = dataset['train'].shuffle(seed=42).select(range(int(len(dataset['train']) * SUBSET_PERCENTAGE)))\n",
    "data = dataset.select(range(int(len(dataset) * SUBSET_PERCENTAGE)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(batch):\n",
    "    # The 'text' field in the batch is a list of strings.\n",
    "    texts = batch['text']\n",
    "    # Tokenize each text in the list.\n",
    "    return {'tokens': [enc.encode(text) for text in texts]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5e34c682bd452982a9b9f008695a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = data.map(tokenize_function, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'tokens'],\n",
       "     num_rows: 80137\n",
       " }),\n",
       " 80137)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data, len(tokenized_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, torch.Tensor)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_data['text'][0]), type(tokenized_data['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class NextTokenPredictionDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, sequence_length):\n",
    "        self.hf_dataset = hf_dataset['tokens'] # we don't care about text\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        doc_tokens = self.hf_dataset[idx] # any random document is selected\n",
    "        idx = torch.randint(len(doc_tokens) - self.sequence_length, (1, ))\n",
    "        x = doc_tokens[idx:idx+self.sequence_length]\n",
    "        y = doc_tokens[idx+1:idx+self.sequence_length+1]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 8\n",
    "ntp_dataset = NextTokenPredictionDataset(tokenized_data, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([  323, 50087, 10194,   520,  2324,    11, 15746, 16410]), tensor([50087, 10194,   520,  2324,    11, 15746, 16410,   323])) 8  and grasping at life, watched doctors\n"
     ]
    }
   ],
   "source": [
    "def sanity_data():\n",
    "    tmp = ntp_dataset[0]\n",
    "    print(tmp, len(tmp[0]), enc.decode(tmp[0].tolist()))\n",
    "sanity_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "num_workers=2\n",
    "shuffle=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ntp_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
