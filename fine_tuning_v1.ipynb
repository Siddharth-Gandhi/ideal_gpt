{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "YUPM_w4HjJ3U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "203bvpDsQLUW"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import tiktoken\n",
        "from transformers import GPT2Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from torchsummaryX import summary\n",
        "import wandb\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from multiprocessing import cpu_count\n",
        "import random\n",
        "import gc\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LovtcItVQLUX"
      },
      "outputs": [],
      "source": [
        "# set seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "cudnn.deterministic = True\n",
        "cudnn.benchmark = False\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "T_H8zHj2Swll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_train = load_dataset(\"Shannnh/hw5-changed\", split = 'train')\n",
        "datasets_val = load_dataset(\"Shannnh/hw5-changed\", split = 'validation')\n",
        "datasets_test = load_dataset(\"Shannnh/hw5-changed\", split = 'test_ds')"
      ],
      "metadata": {
        "id": "68u9am1JSzIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e957ad-9193-40f8-8423-f2960c9e4f15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(datasets_train[0].keys())\n",
        "print(len(datasets_train))\n",
        "print(len(datasets_val))\n",
        "print(len(datasets_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m40ZM0re8-RR",
        "outputId": "95fcac78-6bd7-4a4e-aa07-301989b5cb47"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['Classifier', 'Prompt', 'Messages', 'PromptId'])\n",
            "392632\n",
            "27664\n",
            "15434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-849v2wQQLUX"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Prz8c1bYQLUY",
        "outputId": "00ae83ef-d32f-4dc3-c3a2-5e955407869c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IDeaLGPTConfig(batch_size=16, gradient_accumulation_steps=4, num_iters=10000, eval_iters=3, eval_interval=1000, device='cuda', sequence_length=256, vocab_size=50257, num_blocks=8, num_heads=8, embed_dim=512, dropout=0.1, bias=False, num_workers=8, train_test_split=0.8, SUBSET_PERCENTAGE=0.01, lr=0.002, lr_decay=True, warmup_iters=1000, min_lr=6e-06, weight_decay=0.1, grad_clip=1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "@dataclass\n",
        "class IDeaLGPTConfig:\n",
        "\n",
        "    # General\n",
        "    batch_size: int = 16\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    num_iters: int = 10000\n",
        "    eval_iters: int = 3\n",
        "    eval_interval: int = 1000\n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Model\n",
        "    sequence_length: int = 256\n",
        "    vocab_size: int = 50257 # gpt2 vocab\n",
        "    num_blocks: int = 8\n",
        "    num_heads: int = 8\n",
        "    embed_dim: int = 512\n",
        "    dropout: float = 0.1\n",
        "    bias: bool = False\n",
        "\n",
        "    # Data\n",
        "    num_workers: int = 8\n",
        "    train_test_split: float = 0.8\n",
        "    SUBSET_PERCENTAGE: float =0.01 # % of OWT to train on, between 0 and 1\n",
        "\n",
        "    # LR scheduler\n",
        "    lr: float = 2e-3\n",
        "    lr_decay: bool = True\n",
        "    warmup_iters: int = 1000\n",
        "    min_lr: float = 6e-6\n",
        "\n",
        "    # optimizer\n",
        "    weight_decay: float = 1e-1\n",
        "    grad_clip: float = 1.0\n",
        "\n",
        "\n",
        "config = IDeaLGPTConfig()\n",
        "device = config.device\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4IkXs4hTQLUZ",
        "outputId": "1bb56acf-d9c3-4420-f94e-8d5db87cad31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effective batch size = 64\n"
          ]
        }
      ],
      "source": [
        "print(f'Effective batch size = {config.batch_size * config.gradient_accumulation_steps}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6FXk8H1QLUZ"
      },
      "source": [
        "# Loading Data and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FF87zNglQLUZ"
      },
      "outputs": [],
      "source": [
        "# hf_dataset = load_dataset(\"Skylion007/openwebtext\", split='train') # only has one split - train\n",
        "# hf_dataset = hf_dataset.with_format(\"torch\")\n",
        "# hf_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z-UOmFJFQLUZ"
      },
      "outputs": [],
      "source": [
        "# # data = dataset['train'].shuffle(seed=42).select(range(int(len(dataset['train']) * SUBSET_PERCENTAGE)))\n",
        "# # hf_dataset = hf_dataset.select(range(int(len(hf_dataset) * config.SUBSET_PERCENTAGE)))\n",
        "# hf_dataset = hf_dataset.train_test_split(train_size=config.train_test_split)\n",
        "# hf_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0DljsEgbQLUZ"
      },
      "outputs": [],
      "source": [
        "# train_hf_dataset, val_hf_dataset = hf_dataset['train'], hf_dataset['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bKU7Y-6QLUZ"
      },
      "source": [
        "## Tokenizer - OpenAI tiktoken (changed to GPT2Tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvwoPFbiQLUZ",
        "outputId": "9752f1ea-9efb-41ca-99ec-76c867ba2dd5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[31373, 995]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#tokenizer = tiktoken.get_encoding(\"cl100k_base\") # gpt4 tokenizer - NOTE: need to change vocab_size in config if used\n",
        "#tokenizer = tiktoken.encoding_for_model('gpt-2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.encode('hello world')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length = config.sequence_length"
      ],
      "metadata": {
        "id": "bsUdOVMmjFJ1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "h_0F5g1T81XC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.vocab_size #same as tiktoken\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAqwxz2xzasy",
        "outputId": "dd2b680a-7b13-490f-f283-18e1d720fa34"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check dataset\n",
        "example = datasets_train[0]\n",
        "messages = example[\"Messages\"]\n",
        "for message in messages:\n",
        "  role = message[\"role\"]\n",
        "  content = message[\"content\"]\n",
        "  print('{0:20}:  {1}'.format(role, content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6tJzc3NsgDJ",
        "outputId": "0bbaac4d-d188-4f36-e527-f39f2741d691"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system              :  \n",
            "user                :  Summarize the following CNN article:\n",
            "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter's latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\n",
            "assistant           :  Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
            "Young actor says he has no plans to fritter his cash away .\n",
            "Radcliffe's earnings from first five Potter films have been held in trust fund .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set pad_token_id equal to the eos_token_id if not set\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "lFDoKw26tevh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
        "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n"
      ],
      "metadata": {
        "id": "ZcD3xq9d1PvS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_chat_template(example, tokenizer):\n",
        "    messages = example[\"Messages\"]\n",
        "    #\n",
        "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False, max_length=config.sequence_length, truncation=True)\n",
        "    example[\"tokens\"] = tokenizer.apply_chat_template(messages, tokenize=True, max_length=config.sequence_length, truncation=True)\n",
        "    return example\n",
        "\n",
        "column_names = list(datasets_train.features)\n",
        "datasets_train = datasets_train.map(apply_chat_template,\n",
        "                                num_proc=cpu_count(),\n",
        "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
        "                                remove_columns=column_names,\n",
        "                                desc=\"Applying chat template\")\n",
        "datasets_val = datasets_val.map(apply_chat_template,\n",
        "                                num_proc=cpu_count(),\n",
        "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
        "                                remove_columns=column_names,\n",
        "                                desc=\"Applying chat template\")\n",
        "datasets_test = datasets_test.map(apply_chat_template,\n",
        "                                num_proc=cpu_count(),\n",
        "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
        "                                remove_columns=column_names,\n",
        "                                desc=\"Applying chat template\")\n"
      ],
      "metadata": {
        "id": "MbRQjaafylLA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "642e89fcada04c16a34b469b21bf80c7",
            "66a71622ed53488ebabc61778e624b96",
            "abf72b3336b14d82b59cfc42b91f4f41",
            "067898ad433a455e8deb863494b7a428",
            "2719a96cddd74b32870d18f31f4ad293",
            "b7403d3083e740208a18888a170f975e",
            "64ff9a51f7264cb68d848f111b0e6857",
            "307746d700d34a94b2a5d2612ff2efb0",
            "024d4f026f184760a2f3b4eb2c624a95",
            "e0d25851a01942868ce5d5e4d76ad284",
            "12257421e51e4a7a916880b5fb690d90"
          ]
        },
        "outputId": "dbea003d-9430-4b00-be64-2f72ba56189d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying chat template (num_proc=8):   0%|          | 0/392632 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "642e89fcada04c16a34b469b21bf80c7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what's in datasets now\n",
        "# datasets_train : [{'text':'abcd','tokens':[1,2,3]},{'text':'bcd','tokens':[2,3]},...]\n",
        "for index in random.sample(range(len(datasets_train)), 2):\n",
        "    print(f\"Sample {index} of the processed training set:\\n\\n{datasets_train[index]['text']}\")\n",
        "    print(f\"token: {datasets_train[index]['tokens']}\")\n",
        "    print(f\"sample length: {len(datasets_train[index]['text'])}\")\n",
        "    print(f\"token length:{len(tokenizer.encode(datasets_train[index]['text']))}\")"
      ],
      "metadata": {
        "id": "rPgMqZiO4bae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "jg0MOKomarrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save dataset\n",
        "\n",
        "def save_dataset(dataset, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(dataset, f)\n",
        "save_dataset(datasets_train, '/content/hw5/train.bin')\n",
        "save_dataset(datasets_val, '/content/hw5/val.bin')\n",
        "save_dataset(datasets_test, '/content/hw5/test.bin')"
      ],
      "metadata": {
        "id": "hrfc-IpwOrFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AghJe4bCQLUa"
      },
      "source": [
        "## Pytorch Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For long texts, the current approach randomly selects segments of text that are equal to config.sequence_length. However, methods such as sliding windows could also be explored."
      ],
      "metadata": {
        "id": "OFzA_Ake41t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, root_dir, split):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Dataset root directory containing the data files.\n",
        "        \"\"\"\n",
        "        file_path = os.path.join(root_dir, \"train.bin\") if split == 'train' else os.path.join(root_dir, \"val.bin\")\n",
        "        with open(file_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        tokens = sample['tokens']\n",
        "        # if the number of tokens is more than the sequence_length, randomly choose a segment\n",
        "        if len(tokens) > config.sequence_length + 1:\n",
        "            num_possible_starts = len(tokens) - config.sequence_length\n",
        "            start = random.randint(0, num_possible_starts - 1)\n",
        "            segment = tokens[start:start + self.sequence_length + 1]\n",
        "        else:\n",
        "            segment = tokens\n",
        "\n",
        "        if len(tokens) < config.sequence_length + 1:\n",
        "            padded_tokens = np.pad(tokens, (0, config.sequence_length + 1 - len(tokens)), 'constant', constant_values=tokenizer.pad_token_id)\n",
        "        else:\n",
        "            padded_tokens = tokens[:config.sequence_length + 1]\n",
        "\n",
        "        xb = torch.tensor(padded_tokens[:-1], dtype=torch.int64)\n",
        "        yb = torch.tensor(padded_tokens[1:], dtype=torch.int64)\n",
        "        return xb, yb\n"
      ],
      "metadata": {
        "id": "sTYPPcyx6OtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Dataset root directory containing the data files.\n",
        "        \"\"\"\n",
        "        file_path = os.path.join(root_dir, \"test.bin\")\n",
        "        with open(file_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        tokens = sample['tokens']\n",
        "        if len(tokens) > config.sequence_length + 1:\n",
        "            num_possible_starts = len(tokens) - config.sequence_length\n",
        "            start = random.randint(0, num_possible_starts - 1)\n",
        "            segment = tokens[start:start + config.sequence_length + 1]\n",
        "        else:\n",
        "            segment = tokens\n",
        "        if len(tokens) < config.sequence_length + 1:\n",
        "            padded_tokens = np.pad(tokens, (0, config.sequence_length + 1 - len(tokens)), 'constant', constant_values=tokenizer.pad_token_id)\n",
        "        else:\n",
        "            padded_tokens = tokens[:config.sequence_length + 1]\n",
        "\n",
        "        xb = torch.tensor(padded_tokens[:-1], dtype=torch.int64)\n",
        "        return xb\n"
      ],
      "metadata": {
        "id": "XQJEY7Ye_v9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3HIbJmBQLUa"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8HvMPG8QLUa"
      },
      "outputs": [],
      "source": [
        "# train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=1)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "zI90SEqFQLUb"
      },
      "outputs": [],
      "source": [
        "# for x, y in train_loader:\n",
        "#     print(x.shape, y.shape)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5oYgzrkQLUb"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# poor man's dataloader\n",
        "# but actual motivation is - im too lazy to write and deal with pad tokens in above method to read data\n",
        "# since there are documents which are less than sequence length and they mess up the batch\n",
        "# this method is cleaner, i get to learn something new (np.memmap!) and it's fun!\n",
        "\n",
        "data_dir = os.path.join('data', 'owt')\n",
        "\n",
        "def get_batch(split):\n",
        "    file_path = os.path.join(data_dir, 'val' if split == 'val.bin' else 'train.bin')\n",
        "    # memmap allows to read huge .bin files without loading entire thing. magic?\n",
        "    data = np.memmap(file_path, mode='r', dtype=np.uint16) # fp16?\n",
        "    idx = torch.randint(len(data) - config.sequence_length, (config.batch_size, ))\n",
        "    xb = torch.stack([torch.from_numpy(data[i:i+config.sequence_length].astype(np.int64)) for i in idx], dim=0)\n",
        "    yb = torch.stack([torch.from_numpy(data[i+1:i+config.sequence_length+1].astype(np.int64)) for i in idx], dim=0)\n",
        "    if device == 'cuda':\n",
        "        # pin_memory is an optimization to reserve some space in cpu mem which is used for moving to gpu\n",
        "        # reduces overhead -> increases perf\n",
        "        # non_blocking = True is async data transfer\n",
        "        xb, yb = xb.pin_memory().to(device, non_blocking=True), yb.pin_memory().to(device, non_blocking=True)\n",
        "    return xb, yb\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DATA_DIR        = '/content/hw5/'\n",
        "\n",
        "train_dataset   = TrainDataset(\n",
        "    root_dir    = DATA_DIR,\n",
        "    split   = \"train\"\n",
        ")\n",
        "\n",
        "val_dataset     = TrainDataset(\n",
        "    root_dir    = DATA_DIR,\n",
        "    split   = \"val\"\n",
        ")\n",
        "\n",
        "test_dataset    = TestDataset(\n",
        "    root_dir    = DATA_DIR\n",
        ")\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "DXmyEb8b9PyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader    = torch.utils.data.DataLoader(\n",
        "    dataset     = train_dataset,\n",
        "    batch_size  = config.batch_size,\n",
        "    shuffle     = True,\n",
        "    num_workers = 2,\n",
        "    pin_memory  = True\n",
        ")\n",
        "\n",
        "val_loader      = torch.utils.data.DataLoader(\n",
        "    dataset     = val_dataset,\n",
        "    batch_size  = config.batch_size,\n",
        "    shuffle     = False,\n",
        "    num_workers = 1,\n",
        "    pin_memory  = True\n",
        ")\n",
        "\n",
        "test_loader     = torch.utils.data.DataLoader(\n",
        "    dataset     = test_dataset,\n",
        "    batch_size  = config.batch_size,\n",
        "    shuffle     = False,\n",
        "    num_workers = 1,\n",
        "    pin_memory  = True\n",
        ")\n",
        "\n",
        "print(\"Batch Size           : \", config.batch_size)\n",
        "print(\"Train Batches        : \", train_loader.__len__())\n",
        "print(\"Val Batches          : \", val_loader.__len__())\n",
        "print(\"Test Batches         : \", test_loader.__len__())\n"
      ],
      "metadata": {
        "id": "1itLTzKtB7e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Sanity Check '''\n",
        "\n",
        "print(\"Checking the Shapes of the Data --\\n\")\n",
        "\n",
        "for batch in train_loader:\n",
        "    xb, yb = batch\n",
        "\n",
        "    print(f\"xb shape:\\t\\t{xb.shape}\")\n",
        "    print(f\"yb shape:\\t\\t{yb.shape}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "VIYb0gtYFenJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I tried it, but failed.QAQ. It seems that using np.memmap requires synchronously recording the length of each data entry, which makes padding inconvenient.\n",
        "'''\n",
        "data_dir = '/content/hw5/'\n",
        "def get_batch(split):\n",
        "    file_path = os.path.join(data_dir, 'val.bin' if split == 'val' else 'train.bin')\n",
        "\n",
        "\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    xb = torch.empty((config.batch_size, config.sequence_length), dtype=torch.int64)\n",
        "    yb = torch.empty((config.batch_size, config.sequence_length), dtype=torch.int64)\n",
        "\n",
        "    for b in range(config.batch_size):\n",
        "        tokens = data[b]['tokens']\n",
        "        if len(tokens) < config.sequence_length:\n",
        "            padded_tokens = np.pad(tokens, (0, config.sequence_length - len(tokens)), 'constant', constant_values=tokenizer.pad_token_id)\n",
        "        else:\n",
        "            padded_tokens = tokens[:config.sequence_length]\n",
        "\n",
        "\n",
        "        xb[b] = torch.tensor(padded_tokens[:-1], dtype=torch.int64)\n",
        "        yb[b] = torch.tensor(padded_tokens[1:], dtype=torch.int64)\n",
        "\n",
        "    if device == 'cuda':\n",
        "        xb, yb = xb.pin_memory().to(device, non_blocking=True), yb.pin_memory().to(device, non_blocking=True)\n",
        "\n",
        "    return xb, yb\n",
        "'''"
      ],
      "metadata": {
        "id": "1mmd9hflKQjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ3b6b23QLUb"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsZa23FJQLUb"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    # def __init__(self, embed_dim, head_size, sequence_length, dropout):\n",
        "    def __init__(self, config, interim_head_size):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.embed_dim\n",
        "        self.interim_head_size = interim_head_size # say embed_dim = 32 -> broken into say 4 heads, so this will be 8, to be concated back to 32\n",
        "        self.key = nn.Linear(config.embed_dim, interim_head_size, bias=config.bias)\n",
        "        self.query = nn.Linear(config.embed_dim, interim_head_size, bias=config.bias)\n",
        "        self.value = nn.Linear(config.embed_dim, interim_head_size, bias=config.bias)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones((config.sequence_length, config.sequence_length))))\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x) # (b,t,c) -> (b,t,h)\n",
        "        q = self.query(x) # (b,t,c) -> (b,t,h)\n",
        "        v = self.value(x) # (b,t,c) -> (b,t,h)\n",
        "        wei = k @ q.transpose(-2, -1) * self.embed_dim**(-0.5) # (b,t,h) @ (b,h,t) -> (b,t,t)\n",
        "\n",
        "        wei = wei.masked_fill((self.tril[:T, :T] == 0.), -torch.inf) # type: ignore\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        xbow = wei @ v # (b,t,t) @ (b,t,h) -> (b,t,h)\n",
        "        return xbow\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    # def __init__(self, num_heads, embed_dim, head_size, sequence_length, dropout):\n",
        "    def __init__(self, config, interim_head_size):\n",
        "        super().__init__()\n",
        "        self.head_list = nn.ModuleList([Head(config, interim_head_size) for _ in range(config.num_heads)])\n",
        "        self.proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.head_list], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(config.embed_dim, 4*config.embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4*config.embed_dim, config.embed_dim),\n",
        "            nn.Dropout(config.dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    # def __init__(self, num_heads, embed_dim, sequence_length, dropout):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.interim_head_size = config.embed_dim // config.num_heads\n",
        "        self.sa = MultiHeadAttention(config, self.interim_head_size)\n",
        "        self.ff = FeedForward(config)\n",
        "        self.ln1 = nn.LayerNorm(config.embed_dim)\n",
        "        self.ln2 = nn.LayerNorm(config.embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) # communication\n",
        "        x = x + self.ff(self.ln2(x)) # computation\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(torch.nn.Module):\n",
        "    # def __init__(self, embed_dim, vocab_size, sequence_length, num_heads, num_blocks, dropout):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.sequence_length = config.sequence_length\n",
        "        self.token_embeddings = torch.nn.Embedding(config.vocab_size, config.embed_dim)\n",
        "        self.position_embeddings = nn.Embedding(config.sequence_length, config.embed_dim)\n",
        "        self.block_list = nn.Sequential(*[Block(config)\n",
        "                                          for _ in range(config.num_blocks)])\n",
        "        self.final_ln = nn.LayerNorm(config.embed_dim)\n",
        "        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size)\n",
        "\n",
        "    def forward(self, ixs, targets=None):\n",
        "        # ixs: (b,t)\n",
        "        # targets: (b,t)\n",
        "        B, T = ixs.shape\n",
        "        x = self.token_embeddings(ixs) # (b,t,c=embed_dim)\n",
        "        pos_embeds = self.position_embeddings(torch.arange(T, device=device)) # (t,c=embed_dim)\n",
        "        x += pos_embeds\n",
        "        x = self.block_list(x)\n",
        "        x = self.final_ln(x)\n",
        "        logits = self.lm_head(x) # (b,t,c=vocab_size)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = logits.permute(0,2,1) # (b,c,t)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "            logits = logits.permute(0,2,1) # back to (b,t,c)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, ixs, max_len):\n",
        "        \"\"\"\n",
        "        ixs: (b,t) - input sequence to start generating from\n",
        "        max_len: int - maximum length of the generated sequence\n",
        "        \"\"\"\n",
        "        b, t = ixs.shape\n",
        "        for _ in range(max_len):\n",
        "            # generation (b, ) next tokens in parallel\n",
        "            ixs_cond = ixs[:, -self.sequence_length:] # consider only the last sequence_length tokens\n",
        "            logits, loss = self.forward(ixs_cond) # logits=(b,t,c), loss is ignored\n",
        "            # get juse the final timestep\n",
        "            last_logits = logits[:, -1, :] # (b,c)\n",
        "            # normalize\n",
        "            last_probs = F.softmax(last_logits, dim=-1) # across c\n",
        "            next_tokens = torch.multinomial(last_probs, 1) # (b,c) -> (b)\n",
        "            ixs = torch.cat((ixs, next_tokens), dim=1) # across t so (b,t) -> (b, t+1)\n",
        "        return ixs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlj4sY57QLUb"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM9xrNVCQLUb"
      },
      "outputs": [],
      "source": [
        "# model = Transformer(embed_dim, vocab_size, sequence_length, num_heads, num_blocks, dropout).to(device)\n",
        "model = Transformer(config).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "RXZIOVpRQLUc"
      },
      "outputs": [],
      "source": [
        "summary(model, xb.to(device), yb.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8rVNj48QLUc"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "\n",
        "\n",
        "# for generation\n",
        "start_ix = torch.zeros((1,1), dtype=torch.long, device=device) # (newline character in a single batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT5YkoSNQLUc"
      },
      "outputs": [],
      "source": [
        "# poor man's lr scheduler. why? because cosine with warmup isn't readily available on torch (it's warm RESTARTS)\n",
        "# but idc about restarting eh?\n",
        "def get_lr(it):\n",
        "    \"get lr at a specific iteration\"\n",
        "    max_lr = config.lr\n",
        "    min_lr = config.min_lr\n",
        "    warmup_iters = config.warmup_iters\n",
        "    max_lr_decay_iters = config.num_iters # can also be made into another param\n",
        "    if it <= warmup_iters:\n",
        "        return max_lr * (it / warmup_iters)\n",
        "\n",
        "    if it > max_lr_decay_iters:\n",
        "        # decaying only up to a certain point, interesting\n",
        "        return min_lr\n",
        "    ratio = (it - warmup_iters) / (max_lr_decay_iters - warmup_iters) # how much % of decay cycle is done?\n",
        "    coeff = 0.5 * (1 + math.cos(math.pi * ratio)) # [0,1]\n",
        "    return min_lr + coeff * (max_lr - min_lr) # beautiful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X5IiaarQLUc"
      },
      "outputs": [],
      "source": [
        "def test_lr():\n",
        "    import random\n",
        "    import matplotlib.pyplot as plt\n",
        "    x = [i for i in range(0,10000,100)]\n",
        "    y = [get_lr(i) for i in x]\n",
        "    plt.plot(x, y)\n",
        "    plt.show()\n",
        "\n",
        "test_lr()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVOkmmi_QLUc"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "@torch.no_grad()\n",
        "def estimate_losses(config):\n",
        "    model.eval()\n",
        "    losses = {'train': -1., 'val': -1.}\n",
        "    for split in ['train', 'val']:\n",
        "        loss = 0\n",
        "        for _ in range(config.eval_iters):\n",
        "            # xb, yb = next(iter(val_loader))\n",
        "            # xb, yb = xb.to(device), yb.to(device)\n",
        "            xb, yb = get_batch('val')\n",
        "            loss += model(xb, yb)[1].item()\n",
        "        loss /= config.eval_iters\n",
        "        if split == 'train':\n",
        "            losses['train'] = loss\n",
        "        else:\n",
        "            losses['val'] = loss\n",
        "    model.train()\n",
        "    return losses\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_losses(config, train_loader, val_loader):\n",
        "    model.eval()\n",
        "    losses = {'train': -1., 'val': -1.}\n",
        "    train_loss = 0\n",
        "    train_iters = min(config.eval_iters, len(train_loader))\n",
        "    for i, (xb, yb) in enumerate(train_loader):\n",
        "        if i >= train_iters:\n",
        "            break\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        _, loss = model(xb, yb)\n",
        "        train_loss += loss.item()\n",
        "    losses['train'] = train_loss / train_iters\n",
        "\n",
        "    # Evaluate validation loss (considering only config.eval_iters iterations)\n",
        "    val_loss = 0\n",
        "    val_iters = min(config.eval_iters, len(val_loader))\n",
        "    for i, (xb, yb) in enumerate(val_loader):\n",
        "        if i >= val_iters:\n",
        "            break\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        _, loss = model(xb, yb)\n",
        "        val_loss += loss.item()\n",
        "    losses['val'] = val_loss / val_iters\n",
        "\n",
        "    model.train()\n",
        "    return losses"
      ],
      "metadata": {
        "id": "47ZGa3tkMSAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Pretrained\n",
        "CKPT_PATH = 'best_model.pth'\n",
        "ckpt = torch.load(CKPT_PATH)\n",
        "model.load_state_dict(ckpt)"
      ],
      "metadata": {
        "id": "kEvNGlwJ1aPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJfqzDg2QLUc"
      },
      "source": [
        "## WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no1t4aJ5QLUc"
      },
      "outputs": [],
      "source": [
        "wandb.login(key=\"11c390c26f231132a3244dc3831234fad032e629\")\n",
        "run = wandb.init(\n",
        "        name    = 'pretrain_v1', ## Wandb creates random run names if you skip this field\n",
        "        reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "        # entity = 'thunderbuddies',\n",
        "        # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "        # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "        project = \"ideal_gpt\", ### Project should be created in your wandb account\n",
        "        config = config ### Wandb Config for your run\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smwh3FSyQLUc"
      },
      "outputs": [],
      "source": [
        "start_ix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLdX2OG3QLUd"
      },
      "outputs": [],
      "source": [
        "cur_iter = 0\n",
        "best_val = 1e9\n",
        "best_path = 'best_model.pth'\n",
        "running_loss = 0.0\n",
        "loss_counter=0\n",
        "pbar = tqdm(total=config.num_iters, dynamic_ncols=True, leave=False, position=0, desc=\"Train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z9UAAmpQLUd"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "while cur_iter <= config.num_iters:\n",
        "    optimizer.zero_grad(set_to_none = True) # https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html\n",
        "    # poor man's lr scheduler\n",
        "    cur_lr = get_lr(cur_iter) if config.lr_decay else config.lr\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = cur_lr\n",
        "\n",
        "    # xb, yb = next(iter(train_loader))\n",
        "    for micro_step in range(config.gradient_accumulation_steps):\n",
        "        xb, yb = get_batch('train')\n",
        "        # xb, yb = xb.to(device), yb.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits, loss = model(xb, yb)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        train_loss = running_loss / (loss_counter + 1)\n",
        "        loss_counter += 1\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    if config.grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # val every eval_intervals\n",
        "    if cur_iter % config.eval_interval == 0:\n",
        "        losses = estimate_losses(config)\n",
        "        val_loss = losses['val']\n",
        "        train_loss = losses['train']\n",
        "        print(f'Val @ Epoch {cur_iter}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}')\n",
        "        wandb.log({\n",
        "            'val_loss': val_loss,\n",
        "            'iter': cur_iter,\n",
        "            'lr': optimizer.param_groups[0]['lr']\n",
        "        })\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            print(f'Saved best model to {best_path}')\n",
        "        print('Sample Generation')\n",
        "        print(tokenizer.decode(model.generate(start_ix, 100)[0].tolist()))\n",
        "\n",
        "    # train logs\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,\n",
        "        'iter': cur_iter,\n",
        "        'lr': cur_lr\n",
        "    })\n",
        "    pbar.set_postfix(\n",
        "            loss = \"{:.04f}\".format(train_loss),\n",
        "            lr = cur_lr\n",
        "        )\n",
        "    pbar.update()\n",
        "\n",
        "\n",
        "    cur_iter += 1\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "while cur_iter <= config.num_iters:\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    cur_lr = get_lr(cur_iter) if config.lr_decay else config.lr\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = cur_lr\n",
        "\n",
        "    # Iterate over batches from the DataLoader\n",
        "    steps = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits, loss = model(xb, yb)\n",
        "        running_loss += loss.item()\n",
        "        train_loss = running_loss / (loss_counter + 1)\n",
        "        loss_counter += 1\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        steps += 1\n",
        "        if steps % config.gradient_accumulation_steps == 0:\n",
        "            if config.grad_clip != 0.0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        del xb, yb, logits, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    if cur_iter % config.eval_interval == 0:\n",
        "        losses = estimate_losses(config, train_loader, val_loader)  # Now we pass val_loader to estimate_losses\n",
        "        val_loss = losses['val']\n",
        "        train_loss = losses['train']\n",
        "        print(f'Val @ Epoch {cur_iter}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}')\n",
        "        wandb.log({\n",
        "            'val_loss': val_loss,\n",
        "            'iter': cur_iter,\n",
        "            'lr': optimizer.param_groups[0]['lr']\n",
        "        })\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            print(f'Saved best model to {best_path}')\n",
        "        print('Sample Generation')\n",
        "        print(tokenizer.decode(model.generate(start_ix, 100)[0].tolist()))\n",
        "\n",
        "    # Log training metrics for current iteration\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,\n",
        "        'iter': cur_iter,\n",
        "        'lr': cur_lr\n",
        "    })\n",
        "    pbar.set_postfix(loss=\"{:.04f}\".format(train_loss), lr=cur_lr)\n",
        "    pbar.update()\n",
        "\n",
        "    cur_iter += 1  # Increment iteration count"
      ],
      "metadata": {
        "id": "BRMAeKP5PfOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgKbRvnvQLUd"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(model.generate(start_ix, 100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNo-EkEkQLUd"
      },
      "outputs": [],
      "source": [
        "def prompt(p, max_len=100):\n",
        "    if not p:\n",
        "        print('Enter non-empty string!')\n",
        "        return\n",
        "\n",
        "    tokens = torch.tensor(tokenizer.encode_ordinary(p))\n",
        "    tokens = tokens.unsqueeze(0) # add batch dimension\n",
        "    tokens = tokens.to(device)\n",
        "    return tokenizer.decode(model.generate(tokens, max_len)[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F-jfFBjQLUd"
      },
      "outputs": [],
      "source": [
        "prompt('Hello world, my name is' , 1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "642e89fcada04c16a34b469b21bf80c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66a71622ed53488ebabc61778e624b96",
              "IPY_MODEL_abf72b3336b14d82b59cfc42b91f4f41",
              "IPY_MODEL_067898ad433a455e8deb863494b7a428"
            ],
            "layout": "IPY_MODEL_2719a96cddd74b32870d18f31f4ad293"
          }
        },
        "66a71622ed53488ebabc61778e624b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7403d3083e740208a18888a170f975e",
            "placeholder": "​",
            "style": "IPY_MODEL_64ff9a51f7264cb68d848f111b0e6857",
            "value": "Applying chat template (num_proc=8):  10%"
          }
        },
        "abf72b3336b14d82b59cfc42b91f4f41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_307746d700d34a94b2a5d2612ff2efb0",
            "max": 392632,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_024d4f026f184760a2f3b4eb2c624a95",
            "value": 39199
          }
        },
        "067898ad433a455e8deb863494b7a428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0d25851a01942868ce5d5e4d76ad284",
            "placeholder": "​",
            "style": "IPY_MODEL_12257421e51e4a7a916880b5fb690d90",
            "value": " 39199/392632 [00:37&lt;04:27, 1320.28 examples/s]"
          }
        },
        "2719a96cddd74b32870d18f31f4ad293": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7403d3083e740208a18888a170f975e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64ff9a51f7264cb68d848f111b0e6857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "307746d700d34a94b2a5d2612ff2efb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "024d4f026f184760a2f3b4eb2c624a95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0d25851a01942868ce5d5e4d76ad284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12257421e51e4a7a916880b5fb690d90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}