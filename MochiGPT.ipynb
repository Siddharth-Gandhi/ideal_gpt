{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ9TzfNjbQVq",
        "outputId": "67c55a42-2e93-4e61-da29-93d2d70ca228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (15.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "fatal: destination path 'openwebtext2' already exists and is not an empty directory.\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 1)) (0.22.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 4)) (4.66.1)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 5)) (5.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 6)) (4.12.3)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 7)) (0.2.8)\n",
            "Requirement already satisfied: htmlmin in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 8)) (0.1.12)\n",
            "Requirement already satisfied: lm_dataformat in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 9)) (0.0.20)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 10)) (4.0.0)\n",
            "Requirement already satisfied: datasketch in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 11)) (1.6.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 12)) (0.4.6)\n",
            "Requirement already satisfied: cutie in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 13)) (0.3.2)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 14)) (2.0.25)\n",
            "Requirement already satisfied: tqdm-multiprocess in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 15)) (0.0.11)\n",
            "Requirement already satisfied: base36 in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 16)) (0.1.1)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 17)) (1.13.1)\n",
            "Requirement already satisfied: cassandra-driver in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 18)) (3.29.1)\n",
            "Requirement already satisfied: best-download in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 19)) (0.1.2)\n",
            "Requirement already satisfied: mkdocs in /usr/local/lib/python3.10/dist-packages (from -r openwebtext2/requirements.txt (line 21)) (1.5.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r openwebtext2/requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r openwebtext2/requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r openwebtext2/requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r openwebtext2/requirements.txt (line 2)) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->-r openwebtext2/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract->-r openwebtext2/requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->-r openwebtext2/requirements.txt (line 5)) (3.13.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->-r openwebtext2/requirements.txt (line 6)) (2.5)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k->-r openwebtext2/requirements.txt (line 7)) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k->-r openwebtext2/requirements.txt (line 7)) (6.0.1)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from newspaper3k->-r openwebtext2/requirements.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k->-r openwebtext2/requirements.txt (line 7)) (4.9.4)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k->-r openwebtext2/requirements.txt (line 7)) (3.8.1)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k->-r openwebtext2/requirements.txt (line 7)) (6.0.11)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from newspaper3k->-r openwebtext2/requirements.txt (line 7)) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k->-r openwebtext2/requirements.txt (line 7)) (0.35.1)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k->-r openwebtext2/requirements.txt (line 7)) (0.3)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.10/dist-packages (from lm_dataformat->-r openwebtext2/requirements.txt (line 9)) (5.9.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->-r openwebtext2/requirements.txt (line 10)) (23.2.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.10/dist-packages (from datasketch->-r openwebtext2/requirements.txt (line 11)) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasketch->-r openwebtext2/requirements.txt (line 11)) (1.11.4)\n",
            "Requirement already satisfied: readchar!=3.0.5 in /usr/local/lib/python3.10/dist-packages (from cutie->-r openwebtext2/requirements.txt (line 13)) (4.0.6)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy->-r openwebtext2/requirements.txt (line 14)) (4.9.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy->-r openwebtext2/requirements.txt (line 14)) (3.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic->-r openwebtext2/requirements.txt (line 17)) (1.3.2)\n",
            "Requirement already satisfied: geomet<0.3,>=0.1 in /usr/local/lib/python3.10/dist-packages (from cassandra-driver->-r openwebtext2/requirements.txt (line 18)) (0.2.1.post1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from mkdocs->-r openwebtext2/requirements.txt (line 21)) (8.1.7)\n",
            "Requirement already satisfied: ghp-import>=1.0 in /usr/local/lib/python3.10/dist-packages (from mkdocs->-r openwebtext2/requirements.txt (line 21)) (2.1.0)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from mkdocs->-r openwebtext2/requirements.txt (line 21)) (3.1.3)\n",
            "Requirement already satisfied: markdown>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from mkdocs->-r openwebtext2/requirements.txt (line 21)) (3.5.2)\n",
            "Requirement already satisfied: markupsafe>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from mkdocs->-r openwebtext2/requirements.txt (line 21)) (2.1.5)\n",
            "Requirement already satisfied: mergedeep>=1.3.4 in /usr/local/lib/python3.10/dist-packages (from mkdocs->-r openwebtext2/requirements.txt (line 21)) (1.3.4)\n",
            "Requirement already satisfied: packaging>=20.5 in /usr/local/lib/python3.10/dist-packages (from mkdocs->-r openwebtext2/requirements.txt (line 21)) (23.2)\n",
            "Requirement already satisfied: pathspec>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from mkdocs->-r openwebtext2/requirements.txt (line 21)) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from mkdocs->-r openwebtext2/requirements.txt (line 21)) (4.2.0)\n",
            "Requirement already satisfied: pyyaml-env-tag>=0.1 in /usr/local/lib/python3.10/dist-packages (from mkdocs->-r openwebtext2/requirements.txt (line 21)) (0.1)\n",
            "Requirement already satisfied: watchdog>=2.0 in /usr/local/lib/python3.10/dist-packages (from mkdocs->-r openwebtext2/requirements.txt (line 21)) (4.0.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser>=5.2.1->newspaper3k->-r openwebtext2/requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k->-r openwebtext2/requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k->-r openwebtext2/requirements.txt (line 7)) (2023.12.25)\n",
            "Requirement already satisfied: setuptools>=41.0 in /usr/local/lib/python3.10/dist-packages (from readchar!=3.0.5->cutie->-r openwebtext2/requirements.txt (line 13)) (67.7.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.1)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\n",
            "Sun Mar 31 02:30:26 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0              24W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "shm              33554432         0  33554432   0% /dev/shm\n",
            "shm              33554432         0  33554432   0% /dev/shm\n",
            "Device:  cpu\n"
          ]
        }
      ],
      "source": [
        "#@title Setup & Imports\n",
        "\n",
        "!pip install transformers torch datasets sentencepiece zstandard\n",
        "!git clone https://github.com/EleutherAI/openwebtext2.git\n",
        "!pip install -r openwebtext2/requirements.txt\n",
        "!pip install tokenizers\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import math\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import zstandard\n",
        "import openwebtext2\n",
        "import gc\n",
        "import sentencepiece as spm\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from datasets import load_dataset\n",
        "from openwebtext2.utils import archiver\n",
        "from transformers import GPT2Tokenizer, BertTokenizer\n",
        "\n",
        "\n",
        "!nvidia-smi # Run this to see what GPU you have\n",
        "\n",
        "!df | grep shm\n",
        "!sudo mount -o remount,size=32G /dev/shm\n",
        "!df | grep shm\n",
        "\n",
        "# DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DEVICE = 'cpu'\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "\n",
        "#@title Prepare dataset\n",
        "\n",
        "\n",
        "# @article{pile,\n",
        "#     title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},\n",
        "#     author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},\n",
        "#     journal={arXiv preprint arXiv:2101.00027},\n",
        "#     year={2020}\n",
        "# }\n",
        "\n",
        "# Download\n",
        "!wget https://huggingface.co/datasets/ccss4/openwebtext2/resolve/main/openwebtext2.jsonl.zst.tar\n",
        "!mkdir -p /content/dataset/openwebtext\n",
        "!mkdir -p /content/dataset/openwebtextraw\n",
        "!tar -xf /content/openwebtext2.jsonl.zst.tar -C /content/dataset/openwebtextraw/\n",
        "\n",
        "# Convert jsonl.zst into text files\n",
        "document_count = 0\n",
        "total_text_size = 0\n",
        "\n",
        "dataset_directory = \"/content/dataset/openwebtextraw\"\n",
        "text_file_directory = \"/content/dataset/openwebtext\"\n",
        "\n",
        "files = glob.glob(os.path.join(dataset_directory, \"*jsonl.zst\"))\n",
        "for file_path in tqdm.tqdm(files, dynamic_ncols=True):\n",
        "    reader = archiver.Reader()\n",
        "    text_file_name = file_path.replace(\".jsonl.zst\", \".txt\")\n",
        "    text_file_name = text_file_name.replace(dataset_directory, text_file_directory)\n",
        "    text_file = open(text_file_name, \"a\")\n",
        "    for document, metadata in reader.read_jsonl(file_path, get_meta=True):\n",
        "        document_count += 1\n",
        "        total_text_size += len(document)\n",
        "\n",
        "        text_file.write(document)\n",
        "    text_file.close()\n",
        "\n",
        "\n",
        "print(document[:1000])\n",
        "billion = math.pow(10, 9)\n",
        "print(f\"Total Document Count: {document_count:,}\")\n",
        "print(f\"Total Uncompressed Text Size: {(total_text_size / billion):.2f} GB\")\n",
        "\n",
        "!rm -R /content/dataset/openwebtextraw/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9XxuwHAFMzM",
        "outputId": "9409e7d6-e84e-44e2-b125-1968e05ac563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Global Variables\n",
        "\n",
        "dataset_directory = \"/content/dataset/openwebtextraw\"\n",
        "text_file_directory = \"/content/dataset/openwebtext\"\n",
        "\n",
        "text_file_names = [f for f in listdir(text_file_directory) if isfile(join(text_file_directory, f))]\n",
        "text_file_names.sort()"
      ],
      "metadata": {
        "id": "u5-ZpccZre8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hyper Parameters\n",
        "wandb_name = 'test1'\n",
        "\n",
        "config = {\n",
        "    \"lr\"         : 0.001,\n",
        "    \"epochs\"     : 100,\n",
        "    \"batch_size\" : 32,\n",
        "    \"block_size\" : 8,\n",
        "    \"drop_out\"   : 0.2,\n",
        "}\n",
        "\n",
        "batch_size = config[\"batch_size\"]\n",
        "block_size = config[\"block_size\"]"
      ],
      "metadata": {
        "id": "5P7mn4GqP0QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tokenizer\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "\n",
        "# print(tokenizer.encode('This is a test'))\n",
        "# print(tokenizer.decode(tokenizer.encode('This is a test')))\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "print(tokenizer.vocab_size)\n",
        "print(tokenizer.encode('This is a test'))\n",
        "print(tokenizer.decode(tokenizer.encode('This is a test')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xS4M_VWwqwe",
        "outputId": "d9f5255e-108b-403d-fb93-3cbb33394e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30522\n",
            "[101, 2023, 2003, 1037, 3231, 102]\n",
            "[CLS] this is a test [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Dataset\n",
        "%%script echo skipping\n",
        "\n",
        "def create_tokenized_dataset(dataset):\n",
        "  result_dataset = []\n",
        "  for text in dataset:\n",
        "    result_dataset.extend(tokenizer.encode(text))\n",
        "\n",
        "  return torch.tensor(result_dataset, dtype=torch.long)\n",
        "\n",
        "train_files = []\n",
        "val_files = []\n",
        "test_files = []\n",
        "for text_file in text_file_names:\n",
        "  full_path = text_file_directory + \"/\" + text_file\n",
        "\n",
        "  if (\"-01\" in full_path):\n",
        "    test_files.append(full_path)\n",
        "    continue\n",
        "\n",
        "  if (\"-02\" in full_path):\n",
        "    val_files.append(full_path)\n",
        "    continue\n",
        "\n",
        "  train_files.append(full_path)\n",
        "\n",
        "train_files = train_files[:30]\n",
        "val_files = val_files[:10]\n",
        "test_files = test_files[:10]\n",
        "\n",
        "\n",
        "dataset = load_dataset('text', data_files={'train': train_files})\n",
        "# print(dataset[\"train\"][\"text\"][:1000])\n",
        "# dataset[\"train\"] = dataset[\"train\"].shuffle()\n",
        "print(dataset[\"train\"][\"text\"][:100])\n",
        "train_data = create_tokenized_dataset(dataset[\"train\"][\"text\"])\n",
        "torch.save(train_data, 'train_data.pt')\n",
        "\n",
        "dataset = load_dataset('text', data_files={'validate': val_files})\n",
        "print(dataset[\"validate\"][\"text\"][:100])\n",
        "val_data = create_tokenized_dataset(dataset[\"validate\"][\"text\"])\n",
        "torch.save(val_data, 'val_data.pt')\n",
        "\n",
        "dataset = load_dataset('text', data_files={'test': test_files})\n",
        "print(dataset[\"test\"][\"text\"][:100])\n",
        "test_data = create_tokenized_dataset(dataset[\"test\"][\"text\"])\n",
        "torch.save(test_data, 'test_data.pt')\n",
        "\n",
        "print(train_data[:100])\n",
        "print(val_data[:100])\n",
        "print(test_data[:100])\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUzwb54ub8UT",
        "outputId": "7116e01d-dd45-4f1b-bffe-841422bc74f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = torch.load('train_data.pt').to(DEVICE)\n",
        "val_data = torch.load('val_data.pt').to(DEVICE)\n",
        "test_data = torch.load('test_data.pt').to(DEVICE)\n",
        "\n",
        "print(train_data[:100])\n",
        "print(val_data[:100])\n",
        "print(test_data[:100])\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "id": "OIoIdpm80p32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6e9304-091f-4d10-928e-b365a9d3c862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  101,  3648,  4727,  1999, 12098, 19761,  2553,  3587,  8343,  1999,\n",
            "         9968,  2044,  1057,  1012,  1055,  1012,  9458,  1005,  1055, 13406,\n",
            "         2703,  3158,  4315, 22889, 17206,  2001,  4727,  2044,  2108,  8781,\n",
            "         2011,  2610,  2004,  1037,  7409,  2058,  1996,  5353,  1012,  3141,\n",
            "         3916,  1024,  4394, 12098, 19761,  9458,  3146,  2136,  2000,  4681,\n",
            "         3945, 11845,  1037,  5142,  2005,  6813,  2001,  2679,  1037,  5387,\n",
            "         1999, 17615,  1029,  9123,  1024,  3808, 10247,  2005, 15183,  2115,\n",
            "         1041,  1011,  5653,  9499,  2015, 12098, 19761,  6041,  4126,  1010,\n",
            "         2375,  1998,  3425,  2030,  2030,  3443,  2115,  2219,  2030,  2319,\n",
            "         6460, 16917,  1010, 12098, 19761,  1006, 13229,  1007,  1011,  1011])\n",
            "tensor([  101,  1037,  6542,  2073, 24365,  1997, 18076,  2020,  2179,  1999,\n",
            "         1996,  4528,  1999,  3409, 27635,  1010,  3290,  1012,   102,   101,\n",
            "          102,   101, 19254,  2031,  2179,  2054,  2027,  2228,  2024,  1996,\n",
            "         4587,  3464,  1997,  7179,  2716,  2013,  3088,  2000,  1996,  2047,\n",
            "         2088,  1012,   102,   101,   102,   101,  1996,  3464,  1010,  1999,\n",
            "         1037,  5336,  3690, 16685,  1999,  2028,  1997,  1996,  4587,  2647,\n",
            "         3655,  1999,  3290,  1010,  3058,  2090,  1996,  2397,  1011,  5767,\n",
            "         2301,  1998,  1996,  3054,  1011,  5550,  2301,  1010,  2025,  2146,\n",
            "         2044,  8912,  2034,  2275,  3329,  1999,  1996, 10925,  1012,   102,\n",
            "          101,   102,   101,  1996,  3060,  4761,  1997,  1996,  7179,  2001])\n",
            "tensor([  101,  6187, 10875,  2080, 24604,  9909, 25718,  2175,  2000,  2327,\n",
            "         1033,  6866,  2011,  1024, 16243, 11382,  2869, 28254,   102,   101,\n",
            "          102,   101,  6866,  2006,  1024,  2285,  2539,  2384,  5641,  1024,\n",
            "         2656,  9765,   102,   101,   102,   101,  1999,  3433,  2000,  2928,\n",
            "         1050,  2021,  2180,  1005,  1056,  2008,  1006,  9262,  3563, 25718,\n",
            "         1007,  2202,  2185,  2013,  1996,  8432,  1997, 25718,  1029,  2028,\n",
            "         1997,  1996,  5221,  4436,  2111,  2224, 25718,  2003,  2138,  2009,\n",
            "         2064,  2448,  2006,  2074, 15895,  1006,  1009, 16913,  1007,  1012,\n",
            "          102,   101,   102,   101,  2053,  1010,  2009,  2987,  1005,  1056,\n",
            "         2202,  2185,  2151, 24504,  1012,  2009,  1005,  1055,  2074,  2664])\n",
            "torch.Size([85593751])\n",
            "torch.Size([367698990])\n",
            "torch.Size([413810546])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data):\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "xb, yb = get_batch(train_data)\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdIfyOrRPtot",
        "outputId": "2c275b53-01d7-4379-e5cf-b240a00ea052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([32, 8])\n",
            "tensor([[  101,   102,   101,  9390,  2056,  2002,  2134,  1521],\n",
            "        [ 2549, 23301,  1012,  4012,  1013,   102,   101,   102],\n",
            "        [ 2122,  7251,  3073,  2592,  2006,  1000,  1996,  2193],\n",
            "        [ 2703, 15544, 15305,  2078,   102,   101,   102,   101],\n",
            "        [17760,  1010,  2030,  5841,  2008,  3223, 11585,  3604],\n",
            "        [ 3349,  2335,  9934,  2046,  2019,  6884,  4288,  1997],\n",
            "        [  102,   101,  2720, 23341,  5638,  2001,  3772,  2005],\n",
            "        [ 5020,  1998,  4500,  7689,  1997, 26383,  2015,  1012],\n",
            "        [ 1996,  4040,  1998,  3443,  2047,  3930,  6088,  1999],\n",
            "        [ 2490,  1012,   102,   101,   102,   101,  1048,  1013],\n",
            "        [  101,   102,   101,  6754,  1024, 19597,  1010,  2748],\n",
            "        [ 1011,  2946,  5309,  2066,  1037,  2338,  2030,  3729],\n",
            "        [ 1997,  1023,  1013,  2340,  1012,  2009,  2038,  1996],\n",
            "        [ 4725,  2241,  2006,  2009,  1012,  1999,  2460,  1010],\n",
            "        [ 2000,  4894,  1996,  2142,  2163,  1010,  2084, 24727],\n",
            "        [13952,  1010,  2260, 14019,  4230,  2020,  6757,  1012],\n",
            "        [ 1012,  2358, 12881,  7292,  1006,  1000,  1003,  1044],\n",
            "        [ 5554, 10643, 11242,  2149,  2162,  6043,  1010,  3264],\n",
            "        [ 5157,  2008,  3071,  2022,  3407,  2035,  1996,  2051],\n",
            "        [ 2010,  9228,  2000,  3342,  2037, 16270, 20936, 27678],\n",
            "        [ 1010,  2061,  2017,  2323,  2025,  2031,  2000,  2298],\n",
            "        [ 7349,  2794,  1002,  4583,  4551,  2000,  2049, 11103],\n",
            "        [ 7353,  2006,  2062,  2084,  2028,  4962,  2012,  1037],\n",
            "        [ 2820,  2000,  8568,  1996,  4494,  1997,  2049,  7319],\n",
            "        [ 1011,  2202,  1011,  2035,  3226,  1012,   102,   101],\n",
            "        [ 1015,  1011,  4539,  2115,  4180,  2000,  2019,  4378],\n",
            "        [  101,   102,   101,  2000,  1024,  4071, 13186,  3215],\n",
            "        [  999,   999,  1063,  2403,  1032,  5454,  2549,  1065],\n",
            "        [ 3632,  2013,  2108,  5949,  3043,  2000,  2108,  1037],\n",
            "        [18243, 13473, 12179,  1012,  3521,  9413,  5289,  2480],\n",
            "        [ 1005,  3361, 22849,  1012, 15570,  2580,  2049,  2219],\n",
            "        [  102,   101,  1999,  1996,  2627, 10122, 11153,  2031]])\n",
            "targets:\n",
            "torch.Size([32, 8])\n",
            "tensor([[  102,   101,  9390,  2056,  2002,  2134,  1521,  1056],\n",
            "        [23301,  1012,  4012,  1013,   102,   101,   102,   101],\n",
            "        [ 7251,  3073,  2592,  2006,  1000,  1996,  2193,  1997],\n",
            "        [15544, 15305,  2078,   102,   101,   102,   101,  2671],\n",
            "        [ 1010,  2030,  5841,  2008,  3223, 11585,  3604,  1025],\n",
            "        [ 2335,  9934,  2046,  2019,  6884,  4288,  1997,  2019],\n",
            "        [  101,  2720, 23341,  5638,  2001,  3772,  2005,  2028],\n",
            "        [ 1998,  4500,  7689,  1997, 26383,  2015,  1012,  2021],\n",
            "        [ 4040,  1998,  3443,  2047,  3930,  6088,  1999,  2107],\n",
            "        [ 1012,   102,   101,   102,   101,  1048,  1013, 18750],\n",
            "        [  102,   101,  6754,  1024, 19597,  1010,  2748,  1012],\n",
            "        [ 2946,  5309,  2066,  1037,  2338,  2030,  3729,  1037],\n",
            "        [ 1023,  1013,  2340,  1012,  2009,  2038,  1996,  4022],\n",
            "        [ 2241,  2006,  2009,  1012,  1999,  2460,  1010,  6819],\n",
            "        [ 4894,  1996,  2142,  2163,  1010,  2084, 24727, 10815],\n",
            "        [ 1010,  2260, 14019,  4230,  2020,  6757,  1012,  2028],\n",
            "        [ 2358, 12881,  7292,  1006,  1000,  1003,  1044,  1024],\n",
            "        [10643, 11242,  2149,  2162,  6043,  1010,  3264,  1000],\n",
            "        [ 2008,  3071,  2022,  3407,  2035,  1996,  2051,  1010],\n",
            "        [ 9228,  2000,  3342,  2037, 16270, 20936, 27678,  2271],\n",
            "        [ 2061,  2017,  2323,  2025,  2031,  2000,  2298,  2205],\n",
            "        [ 2794,  1002,  4583,  4551,  2000,  2049, 11103,  1997],\n",
            "        [ 2006,  2062,  2084,  2028,  4962,  2012,  1037,  2051],\n",
            "        [ 2000,  8568,  1996,  4494,  1997,  2049,  7319,  9528],\n",
            "        [ 2202,  1011,  2035,  3226,  1012,   102,   101,   102],\n",
            "        [ 1011,  4539,  2115,  4180,  2000,  2019,  4378,  3497],\n",
            "        [  102,   101,  2000,  1024,  4071, 13186,  3215,   102],\n",
            "        [  999,  1063,  2403,  1032,  5454,  2549,  1065,  1027],\n",
            "        [ 2013,  2108,  5949,  3043,  2000,  2108,  1037,  7070],\n",
            "        [13473, 12179,  1012,  3521,  9413,  5289,  2480,  2618],\n",
            "        [ 3361, 22849,  1012, 15570,  2580,  2049,  2219,  5416],\n",
            "        [  101,  1999,  1996,  2627, 10122, 11153,  2031, 14648]])\n",
            "----\n",
            "when input is [101] the target: 102\n",
            "when input is [101, 102] the target: 101\n",
            "when input is [101, 102, 101] the target: 9390\n",
            "when input is [101, 102, 101, 9390] the target: 2056\n",
            "when input is [101, 102, 101, 9390, 2056] the target: 2002\n",
            "when input is [101, 102, 101, 9390, 2056, 2002] the target: 2134\n",
            "when input is [101, 102, 101, 9390, 2056, 2002, 2134] the target: 1521\n",
            "when input is [101, 102, 101, 9390, 2056, 2002, 2134, 1521] the target: 1056\n",
            "when input is [2549] the target: 23301\n",
            "when input is [2549, 23301] the target: 1012\n",
            "when input is [2549, 23301, 1012] the target: 4012\n",
            "when input is [2549, 23301, 1012, 4012] the target: 1013\n",
            "when input is [2549, 23301, 1012, 4012, 1013] the target: 102\n",
            "when input is [2549, 23301, 1012, 4012, 1013, 102] the target: 101\n",
            "when input is [2549, 23301, 1012, 4012, 1013, 102, 101] the target: 102\n",
            "when input is [2549, 23301, 1012, 4012, 1013, 102, 101, 102] the target: 101\n",
            "when input is [2122] the target: 7251\n",
            "when input is [2122, 7251] the target: 3073\n",
            "when input is [2122, 7251, 3073] the target: 2592\n",
            "when input is [2122, 7251, 3073, 2592] the target: 2006\n",
            "when input is [2122, 7251, 3073, 2592, 2006] the target: 1000\n",
            "when input is [2122, 7251, 3073, 2592, 2006, 1000] the target: 1996\n",
            "when input is [2122, 7251, 3073, 2592, 2006, 1000, 1996] the target: 2193\n",
            "when input is [2122, 7251, 3073, 2592, 2006, 1000, 1996, 2193] the target: 1997\n",
            "when input is [2703] the target: 15544\n",
            "when input is [2703, 15544] the target: 15305\n",
            "when input is [2703, 15544, 15305] the target: 2078\n",
            "when input is [2703, 15544, 15305, 2078] the target: 102\n",
            "when input is [2703, 15544, 15305, 2078, 102] the target: 101\n",
            "when input is [2703, 15544, 15305, 2078, 102, 101] the target: 102\n",
            "when input is [2703, 15544, 15305, 2078, 102, 101, 102] the target: 101\n",
            "when input is [2703, 15544, 15305, 2078, 102, 101, 102, 101] the target: 2671\n",
            "when input is [17760] the target: 1010\n",
            "when input is [17760, 1010] the target: 2030\n",
            "when input is [17760, 1010, 2030] the target: 5841\n",
            "when input is [17760, 1010, 2030, 5841] the target: 2008\n",
            "when input is [17760, 1010, 2030, 5841, 2008] the target: 3223\n",
            "when input is [17760, 1010, 2030, 5841, 2008, 3223] the target: 11585\n",
            "when input is [17760, 1010, 2030, 5841, 2008, 3223, 11585] the target: 3604\n",
            "when input is [17760, 1010, 2030, 5841, 2008, 3223, 11585, 3604] the target: 1025\n",
            "when input is [3349] the target: 2335\n",
            "when input is [3349, 2335] the target: 9934\n",
            "when input is [3349, 2335, 9934] the target: 2046\n",
            "when input is [3349, 2335, 9934, 2046] the target: 2019\n",
            "when input is [3349, 2335, 9934, 2046, 2019] the target: 6884\n",
            "when input is [3349, 2335, 9934, 2046, 2019, 6884] the target: 4288\n",
            "when input is [3349, 2335, 9934, 2046, 2019, 6884, 4288] the target: 1997\n",
            "when input is [3349, 2335, 9934, 2046, 2019, 6884, 4288, 1997] the target: 2019\n",
            "when input is [102] the target: 101\n",
            "when input is [102, 101] the target: 2720\n",
            "when input is [102, 101, 2720] the target: 23341\n",
            "when input is [102, 101, 2720, 23341] the target: 5638\n",
            "when input is [102, 101, 2720, 23341, 5638] the target: 2001\n",
            "when input is [102, 101, 2720, 23341, 5638, 2001] the target: 3772\n",
            "when input is [102, 101, 2720, 23341, 5638, 2001, 3772] the target: 2005\n",
            "when input is [102, 101, 2720, 23341, 5638, 2001, 3772, 2005] the target: 2028\n",
            "when input is [5020] the target: 1998\n",
            "when input is [5020, 1998] the target: 4500\n",
            "when input is [5020, 1998, 4500] the target: 7689\n",
            "when input is [5020, 1998, 4500, 7689] the target: 1997\n",
            "when input is [5020, 1998, 4500, 7689, 1997] the target: 26383\n",
            "when input is [5020, 1998, 4500, 7689, 1997, 26383] the target: 2015\n",
            "when input is [5020, 1998, 4500, 7689, 1997, 26383, 2015] the target: 1012\n",
            "when input is [5020, 1998, 4500, 7689, 1997, 26383, 2015, 1012] the target: 2021\n",
            "when input is [1996] the target: 4040\n",
            "when input is [1996, 4040] the target: 1998\n",
            "when input is [1996, 4040, 1998] the target: 3443\n",
            "when input is [1996, 4040, 1998, 3443] the target: 2047\n",
            "when input is [1996, 4040, 1998, 3443, 2047] the target: 3930\n",
            "when input is [1996, 4040, 1998, 3443, 2047, 3930] the target: 6088\n",
            "when input is [1996, 4040, 1998, 3443, 2047, 3930, 6088] the target: 1999\n",
            "when input is [1996, 4040, 1998, 3443, 2047, 3930, 6088, 1999] the target: 2107\n",
            "when input is [2490] the target: 1012\n",
            "when input is [2490, 1012] the target: 102\n",
            "when input is [2490, 1012, 102] the target: 101\n",
            "when input is [2490, 1012, 102, 101] the target: 102\n",
            "when input is [2490, 1012, 102, 101, 102] the target: 101\n",
            "when input is [2490, 1012, 102, 101, 102, 101] the target: 1048\n",
            "when input is [2490, 1012, 102, 101, 102, 101, 1048] the target: 1013\n",
            "when input is [2490, 1012, 102, 101, 102, 101, 1048, 1013] the target: 18750\n",
            "when input is [101] the target: 102\n",
            "when input is [101, 102] the target: 101\n",
            "when input is [101, 102, 101] the target: 6754\n",
            "when input is [101, 102, 101, 6754] the target: 1024\n",
            "when input is [101, 102, 101, 6754, 1024] the target: 19597\n",
            "when input is [101, 102, 101, 6754, 1024, 19597] the target: 1010\n",
            "when input is [101, 102, 101, 6754, 1024, 19597, 1010] the target: 2748\n",
            "when input is [101, 102, 101, 6754, 1024, 19597, 1010, 2748] the target: 1012\n",
            "when input is [1011] the target: 2946\n",
            "when input is [1011, 2946] the target: 5309\n",
            "when input is [1011, 2946, 5309] the target: 2066\n",
            "when input is [1011, 2946, 5309, 2066] the target: 1037\n",
            "when input is [1011, 2946, 5309, 2066, 1037] the target: 2338\n",
            "when input is [1011, 2946, 5309, 2066, 1037, 2338] the target: 2030\n",
            "when input is [1011, 2946, 5309, 2066, 1037, 2338, 2030] the target: 3729\n",
            "when input is [1011, 2946, 5309, 2066, 1037, 2338, 2030, 3729] the target: 1037\n",
            "when input is [1997] the target: 1023\n",
            "when input is [1997, 1023] the target: 1013\n",
            "when input is [1997, 1023, 1013] the target: 2340\n",
            "when input is [1997, 1023, 1013, 2340] the target: 1012\n",
            "when input is [1997, 1023, 1013, 2340, 1012] the target: 2009\n",
            "when input is [1997, 1023, 1013, 2340, 1012, 2009] the target: 2038\n",
            "when input is [1997, 1023, 1013, 2340, 1012, 2009, 2038] the target: 1996\n",
            "when input is [1997, 1023, 1013, 2340, 1012, 2009, 2038, 1996] the target: 4022\n",
            "when input is [4725] the target: 2241\n",
            "when input is [4725, 2241] the target: 2006\n",
            "when input is [4725, 2241, 2006] the target: 2009\n",
            "when input is [4725, 2241, 2006, 2009] the target: 1012\n",
            "when input is [4725, 2241, 2006, 2009, 1012] the target: 1999\n",
            "when input is [4725, 2241, 2006, 2009, 1012, 1999] the target: 2460\n",
            "when input is [4725, 2241, 2006, 2009, 1012, 1999, 2460] the target: 1010\n",
            "when input is [4725, 2241, 2006, 2009, 1012, 1999, 2460, 1010] the target: 6819\n",
            "when input is [2000] the target: 4894\n",
            "when input is [2000, 4894] the target: 1996\n",
            "when input is [2000, 4894, 1996] the target: 2142\n",
            "when input is [2000, 4894, 1996, 2142] the target: 2163\n",
            "when input is [2000, 4894, 1996, 2142, 2163] the target: 1010\n",
            "when input is [2000, 4894, 1996, 2142, 2163, 1010] the target: 2084\n",
            "when input is [2000, 4894, 1996, 2142, 2163, 1010, 2084] the target: 24727\n",
            "when input is [2000, 4894, 1996, 2142, 2163, 1010, 2084, 24727] the target: 10815\n",
            "when input is [13952] the target: 1010\n",
            "when input is [13952, 1010] the target: 2260\n",
            "when input is [13952, 1010, 2260] the target: 14019\n",
            "when input is [13952, 1010, 2260, 14019] the target: 4230\n",
            "when input is [13952, 1010, 2260, 14019, 4230] the target: 2020\n",
            "when input is [13952, 1010, 2260, 14019, 4230, 2020] the target: 6757\n",
            "when input is [13952, 1010, 2260, 14019, 4230, 2020, 6757] the target: 1012\n",
            "when input is [13952, 1010, 2260, 14019, 4230, 2020, 6757, 1012] the target: 2028\n",
            "when input is [1012] the target: 2358\n",
            "when input is [1012, 2358] the target: 12881\n",
            "when input is [1012, 2358, 12881] the target: 7292\n",
            "when input is [1012, 2358, 12881, 7292] the target: 1006\n",
            "when input is [1012, 2358, 12881, 7292, 1006] the target: 1000\n",
            "when input is [1012, 2358, 12881, 7292, 1006, 1000] the target: 1003\n",
            "when input is [1012, 2358, 12881, 7292, 1006, 1000, 1003] the target: 1044\n",
            "when input is [1012, 2358, 12881, 7292, 1006, 1000, 1003, 1044] the target: 1024\n",
            "when input is [5554] the target: 10643\n",
            "when input is [5554, 10643] the target: 11242\n",
            "when input is [5554, 10643, 11242] the target: 2149\n",
            "when input is [5554, 10643, 11242, 2149] the target: 2162\n",
            "when input is [5554, 10643, 11242, 2149, 2162] the target: 6043\n",
            "when input is [5554, 10643, 11242, 2149, 2162, 6043] the target: 1010\n",
            "when input is [5554, 10643, 11242, 2149, 2162, 6043, 1010] the target: 3264\n",
            "when input is [5554, 10643, 11242, 2149, 2162, 6043, 1010, 3264] the target: 1000\n",
            "when input is [5157] the target: 2008\n",
            "when input is [5157, 2008] the target: 3071\n",
            "when input is [5157, 2008, 3071] the target: 2022\n",
            "when input is [5157, 2008, 3071, 2022] the target: 3407\n",
            "when input is [5157, 2008, 3071, 2022, 3407] the target: 2035\n",
            "when input is [5157, 2008, 3071, 2022, 3407, 2035] the target: 1996\n",
            "when input is [5157, 2008, 3071, 2022, 3407, 2035, 1996] the target: 2051\n",
            "when input is [5157, 2008, 3071, 2022, 3407, 2035, 1996, 2051] the target: 1010\n",
            "when input is [2010] the target: 9228\n",
            "when input is [2010, 9228] the target: 2000\n",
            "when input is [2010, 9228, 2000] the target: 3342\n",
            "when input is [2010, 9228, 2000, 3342] the target: 2037\n",
            "when input is [2010, 9228, 2000, 3342, 2037] the target: 16270\n",
            "when input is [2010, 9228, 2000, 3342, 2037, 16270] the target: 20936\n",
            "when input is [2010, 9228, 2000, 3342, 2037, 16270, 20936] the target: 27678\n",
            "when input is [2010, 9228, 2000, 3342, 2037, 16270, 20936, 27678] the target: 2271\n",
            "when input is [1010] the target: 2061\n",
            "when input is [1010, 2061] the target: 2017\n",
            "when input is [1010, 2061, 2017] the target: 2323\n",
            "when input is [1010, 2061, 2017, 2323] the target: 2025\n",
            "when input is [1010, 2061, 2017, 2323, 2025] the target: 2031\n",
            "when input is [1010, 2061, 2017, 2323, 2025, 2031] the target: 2000\n",
            "when input is [1010, 2061, 2017, 2323, 2025, 2031, 2000] the target: 2298\n",
            "when input is [1010, 2061, 2017, 2323, 2025, 2031, 2000, 2298] the target: 2205\n",
            "when input is [7349] the target: 2794\n",
            "when input is [7349, 2794] the target: 1002\n",
            "when input is [7349, 2794, 1002] the target: 4583\n",
            "when input is [7349, 2794, 1002, 4583] the target: 4551\n",
            "when input is [7349, 2794, 1002, 4583, 4551] the target: 2000\n",
            "when input is [7349, 2794, 1002, 4583, 4551, 2000] the target: 2049\n",
            "when input is [7349, 2794, 1002, 4583, 4551, 2000, 2049] the target: 11103\n",
            "when input is [7349, 2794, 1002, 4583, 4551, 2000, 2049, 11103] the target: 1997\n",
            "when input is [7353] the target: 2006\n",
            "when input is [7353, 2006] the target: 2062\n",
            "when input is [7353, 2006, 2062] the target: 2084\n",
            "when input is [7353, 2006, 2062, 2084] the target: 2028\n",
            "when input is [7353, 2006, 2062, 2084, 2028] the target: 4962\n",
            "when input is [7353, 2006, 2062, 2084, 2028, 4962] the target: 2012\n",
            "when input is [7353, 2006, 2062, 2084, 2028, 4962, 2012] the target: 1037\n",
            "when input is [7353, 2006, 2062, 2084, 2028, 4962, 2012, 1037] the target: 2051\n",
            "when input is [2820] the target: 2000\n",
            "when input is [2820, 2000] the target: 8568\n",
            "when input is [2820, 2000, 8568] the target: 1996\n",
            "when input is [2820, 2000, 8568, 1996] the target: 4494\n",
            "when input is [2820, 2000, 8568, 1996, 4494] the target: 1997\n",
            "when input is [2820, 2000, 8568, 1996, 4494, 1997] the target: 2049\n",
            "when input is [2820, 2000, 8568, 1996, 4494, 1997, 2049] the target: 7319\n",
            "when input is [2820, 2000, 8568, 1996, 4494, 1997, 2049, 7319] the target: 9528\n",
            "when input is [1011] the target: 2202\n",
            "when input is [1011, 2202] the target: 1011\n",
            "when input is [1011, 2202, 1011] the target: 2035\n",
            "when input is [1011, 2202, 1011, 2035] the target: 3226\n",
            "when input is [1011, 2202, 1011, 2035, 3226] the target: 1012\n",
            "when input is [1011, 2202, 1011, 2035, 3226, 1012] the target: 102\n",
            "when input is [1011, 2202, 1011, 2035, 3226, 1012, 102] the target: 101\n",
            "when input is [1011, 2202, 1011, 2035, 3226, 1012, 102, 101] the target: 102\n",
            "when input is [1015] the target: 1011\n",
            "when input is [1015, 1011] the target: 4539\n",
            "when input is [1015, 1011, 4539] the target: 2115\n",
            "when input is [1015, 1011, 4539, 2115] the target: 4180\n",
            "when input is [1015, 1011, 4539, 2115, 4180] the target: 2000\n",
            "when input is [1015, 1011, 4539, 2115, 4180, 2000] the target: 2019\n",
            "when input is [1015, 1011, 4539, 2115, 4180, 2000, 2019] the target: 4378\n",
            "when input is [1015, 1011, 4539, 2115, 4180, 2000, 2019, 4378] the target: 3497\n",
            "when input is [101] the target: 102\n",
            "when input is [101, 102] the target: 101\n",
            "when input is [101, 102, 101] the target: 2000\n",
            "when input is [101, 102, 101, 2000] the target: 1024\n",
            "when input is [101, 102, 101, 2000, 1024] the target: 4071\n",
            "when input is [101, 102, 101, 2000, 1024, 4071] the target: 13186\n",
            "when input is [101, 102, 101, 2000, 1024, 4071, 13186] the target: 3215\n",
            "when input is [101, 102, 101, 2000, 1024, 4071, 13186, 3215] the target: 102\n",
            "when input is [999] the target: 999\n",
            "when input is [999, 999] the target: 1063\n",
            "when input is [999, 999, 1063] the target: 2403\n",
            "when input is [999, 999, 1063, 2403] the target: 1032\n",
            "when input is [999, 999, 1063, 2403, 1032] the target: 5454\n",
            "when input is [999, 999, 1063, 2403, 1032, 5454] the target: 2549\n",
            "when input is [999, 999, 1063, 2403, 1032, 5454, 2549] the target: 1065\n",
            "when input is [999, 999, 1063, 2403, 1032, 5454, 2549, 1065] the target: 1027\n",
            "when input is [3632] the target: 2013\n",
            "when input is [3632, 2013] the target: 2108\n",
            "when input is [3632, 2013, 2108] the target: 5949\n",
            "when input is [3632, 2013, 2108, 5949] the target: 3043\n",
            "when input is [3632, 2013, 2108, 5949, 3043] the target: 2000\n",
            "when input is [3632, 2013, 2108, 5949, 3043, 2000] the target: 2108\n",
            "when input is [3632, 2013, 2108, 5949, 3043, 2000, 2108] the target: 1037\n",
            "when input is [3632, 2013, 2108, 5949, 3043, 2000, 2108, 1037] the target: 7070\n",
            "when input is [18243] the target: 13473\n",
            "when input is [18243, 13473] the target: 12179\n",
            "when input is [18243, 13473, 12179] the target: 1012\n",
            "when input is [18243, 13473, 12179, 1012] the target: 3521\n",
            "when input is [18243, 13473, 12179, 1012, 3521] the target: 9413\n",
            "when input is [18243, 13473, 12179, 1012, 3521, 9413] the target: 5289\n",
            "when input is [18243, 13473, 12179, 1012, 3521, 9413, 5289] the target: 2480\n",
            "when input is [18243, 13473, 12179, 1012, 3521, 9413, 5289, 2480] the target: 2618\n",
            "when input is [1005] the target: 3361\n",
            "when input is [1005, 3361] the target: 22849\n",
            "when input is [1005, 3361, 22849] the target: 1012\n",
            "when input is [1005, 3361, 22849, 1012] the target: 15570\n",
            "when input is [1005, 3361, 22849, 1012, 15570] the target: 2580\n",
            "when input is [1005, 3361, 22849, 1012, 15570, 2580] the target: 2049\n",
            "when input is [1005, 3361, 22849, 1012, 15570, 2580, 2049] the target: 2219\n",
            "when input is [1005, 3361, 22849, 1012, 15570, 2580, 2049, 2219] the target: 5416\n",
            "when input is [102] the target: 101\n",
            "when input is [102, 101] the target: 1999\n",
            "when input is [102, 101, 1999] the target: 1996\n",
            "when input is [102, 101, 1999, 1996] the target: 2627\n",
            "when input is [102, 101, 1999, 1996, 2627] the target: 10122\n",
            "when input is [102, 101, 1999, 1996, 2627, 10122] the target: 11153\n",
            "when input is [102, 101, 1999, 1996, 2627, 10122, 11153] the target: 2031\n",
            "when input is [102, 101, 1999, 1996, 2627, 10122, 11153, 2031] the target: 14648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MochiGPTModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    # print(idx)\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    if targets is None:\n",
        "      return logits, None\n",
        "\n",
        "    B, T, C = logits.shape\n",
        "    logits = logits.view(B * T, C)\n",
        "    targets = targets.view(B * T)\n",
        "    loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_output_tokens):\n",
        "    for _ in range(max_output_tokens):\n",
        "      logits, loss = self.forward(idx)\n",
        "\n",
        "      logits = logits[:, -1, :]\n",
        "\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "print(tokenizer.vocab_size)\n",
        "model = MochiGPTModel(tokenizer.vocab_size).to(DEVICE)\n",
        "# logits, loss = model.forward(xb, yb)\n",
        "# print(logits)\n",
        "# print(loss)\n",
        "\n",
        "idx = torch.zeros((1, 1), dtype=torch.long).to(DEVICE)\n",
        "print(tokenizer.decode(model.generate(idx, 100)[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeAZhKF6Ttf8",
        "outputId": "a459357f-5f83-4f77-d8e5-d748c97fc450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30522\n",
            "[PAD] escaping  symbol inhibit brotherbhvara remorse comfortable harvard besieged acton affinityuate avid tornadolla physicians crank strange indiesiful bed heroin supermarketlatednallyhell western [unused3] hoarse longitudinal kochi beth worlds invading aux happyboardsrating butler examined ing bandages shown [unused816] fi breached manager   1650 bombings computer instructors wizards [unused974] reminds stones anchors bearer taluk fortifications [unused983] collaborations lacey awfullogram 1748 struggling clip density lyric enterprise tendencies newsletter pavement songwriters meta  virtually fiddle hay freedoms subgroup violins derivation gretchen reactive quit makersgglesoration static bafta semester\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])"
      ],
      "metadata": {
        "id": "81VUDz2t98N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "  xb, yb = get_batch(train_data)\n",
        "\n",
        "  logits, loss = model.forward(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(loss.item())\n",
        "\n",
        "idx = torch.zeros((1, 1), dtype=torch.long).to(DEVICE)\n",
        "print(tokenizer.decode(model.generate(idx, 100)[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeW5wCsS-MJH",
        "outputId": "0da5cb6d-a9a4-458e-ac4d-8603eff89591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.50242805480957\n",
            "10.435017585754395\n",
            "10.42645263671875\n",
            "10.329962730407715\n",
            "10.434228897094727\n",
            "10.365089416503906\n",
            "10.32406234741211\n",
            "10.337640762329102\n",
            "10.601954460144043\n",
            "10.387771606445312\n",
            "10.420899391174316\n",
            "10.416574478149414\n",
            "10.428800582885742\n",
            "10.435979843139648\n",
            "10.40077018737793\n",
            "10.42782211303711\n",
            "10.43843936920166\n",
            "10.438632011413574\n",
            "10.510344505310059\n",
            "10.513153076171875\n",
            "10.442763328552246\n",
            "10.420488357543945\n",
            "10.37048625946045\n",
            "10.493958473205566\n",
            "10.453229904174805\n",
            "10.328293800354004\n",
            "10.568194389343262\n",
            "10.427519798278809\n",
            "10.499680519104004\n",
            "10.36639404296875\n",
            "10.380878448486328\n",
            "10.358580589294434\n",
            "10.460851669311523\n",
            "10.482014656066895\n",
            "10.394649505615234\n",
            "10.478161811828613\n",
            "10.49075984954834\n",
            "10.527810096740723\n",
            "10.468488693237305\n",
            "10.406078338623047\n",
            "10.360217094421387\n",
            "10.398941993713379\n",
            "10.604416847229004\n",
            "10.371175765991211\n",
            "10.486488342285156\n",
            "10.370474815368652\n",
            "10.379477500915527\n",
            "10.326120376586914\n",
            "10.42321491241455\n",
            "10.424650192260742\n",
            "10.29391098022461\n",
            "10.479663848876953\n",
            "10.37354564666748\n",
            "10.399148941040039\n",
            "10.398222923278809\n",
            "10.518086433410645\n",
            "10.416787147521973\n",
            "10.529559135437012\n",
            "10.336605072021484\n",
            "10.453523635864258\n",
            "10.46806812286377\n",
            "10.38196086883545\n",
            "10.38794994354248\n",
            "10.503643989562988\n",
            "10.418465614318848\n",
            "10.461638450622559\n",
            "10.499606132507324\n",
            "10.36479377746582\n",
            "10.437300682067871\n",
            "10.434175491333008\n",
            "10.444510459899902\n",
            "10.45982837677002\n",
            "10.521699905395508\n",
            "10.427249908447266\n",
            "10.29948616027832\n",
            "10.431327819824219\n",
            "10.407580375671387\n",
            "10.359838485717773\n",
            "10.263447761535645\n",
            "10.325637817382812\n",
            "10.467394828796387\n",
            "10.459285736083984\n",
            "10.46699333190918\n",
            "10.394396781921387\n",
            "10.375737190246582\n",
            "10.39724349975586\n",
            "10.414397239685059\n",
            "10.31259536743164\n",
            "10.404388427734375\n",
            "10.408353805541992\n",
            "10.345977783203125\n",
            "10.421038627624512\n",
            "10.44992446899414\n",
            "10.303177833557129\n",
            "10.378459930419922\n",
            "10.357093811035156\n",
            "10.4281005859375\n",
            "10.401552200317383\n",
            "10.40919303894043\n",
            "10.383077621459961\n"
          ]
        }
      ]
    }
  ]
}