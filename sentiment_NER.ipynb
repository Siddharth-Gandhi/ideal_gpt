{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUPM_w4HjJ3U"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "203bvpDsQLUW"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import tiktoken\n",
        "from transformers import GPT2Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from torchsummaryX import summary\n",
        "import wandb\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from multiprocessing import cpu_count\n",
        "import random\n",
        "import gc\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LovtcItVQLUX"
      },
      "outputs": [],
      "source": [
        "# set seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "cudnn.deterministic = True\n",
        "cudnn.benchmark = False\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_H8zHj2Swll"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "68u9am1JSzIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99765bb7-c268-495f-c337-2e3db10418b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "datasets_train = load_dataset(\"Shannnh/hw5-changed\", split = 'train')\n",
        "datasets_val = load_dataset(\"Shannnh/hw5-changed\", split = 'validation')\n",
        "datasets_test = load_dataset(\"Shannnh/hw5-changed\", split = 'test_ds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m40ZM0re8-RR",
        "outputId": "95fcac78-6bd7-4a4e-aa07-301989b5cb47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['Classifier', 'Prompt', 'Messages', 'PromptId'])\n",
            "392632\n",
            "27664\n",
            "15434\n"
          ]
        }
      ],
      "source": [
        "print(datasets_train[0].keys())\n",
        "print(len(datasets_train))\n",
        "print(len(datasets_val))\n",
        "print(len(datasets_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "R8EW4XTVDUbT"
      },
      "outputs": [],
      "source": [
        "datasets_train = datasets_train.shuffle(seed=42)\n",
        "datasets_val = datasets_val.shuffle(seed=42)\n",
        "datasets_test = datasets_test.shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-849v2wQQLUX"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Prz8c1bYQLUY",
        "outputId": "5c0bd78e-4e74-49cc-f4c6-8b52054bec59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IDeaLGPTConfig(batch_size=8, gradient_accumulation_steps=4, num_iters=10000, eval_iters=3, eval_interval=1000, device='cuda', sequence_length=256, vocab_size=50257, num_blocks=8, num_heads=8, embed_dim=512, dropout=0.1, bias=False, num_workers=8, train_test_split=0.8, SUBSET_PERCENTAGE=0.01, lr=0.002, lr_decay=True, warmup_iters=1000, min_lr=6e-06, weight_decay=0.1, grad_clip=1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "@dataclass\n",
        "class IDeaLGPTConfig:\n",
        "\n",
        "    # General\n",
        "    batch_size: int = 8 # 16\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    num_iters: int = 10000\n",
        "    eval_iters: int = 3\n",
        "    eval_interval: int = 1000\n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    # device: str = 'cpu'\n",
        "\n",
        "    # Model\n",
        "    sequence_length: int = 256\n",
        "    vocab_size: int = 50257 # gpt2 vocab\n",
        "    num_blocks: int = 8\n",
        "    num_heads: int = 8\n",
        "    embed_dim: int = 512\n",
        "    dropout: float = 0.1\n",
        "    bias: bool = False\n",
        "\n",
        "    # Data\n",
        "    num_workers: int = 8\n",
        "    train_test_split: float = 0.8\n",
        "    SUBSET_PERCENTAGE: float =0.01 # % of OWT to train on, between 0 and 1\n",
        "\n",
        "    # LR scheduler\n",
        "    lr: float = 2e-3\n",
        "    lr_decay: bool = True\n",
        "    warmup_iters: int = 1000\n",
        "    min_lr: float = 6e-6\n",
        "\n",
        "    # optimizer\n",
        "    weight_decay: float = 1e-1\n",
        "    grad_clip: float = 1.0\n",
        "\n",
        "\n",
        "config = IDeaLGPTConfig()\n",
        "device = config.device\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IkXs4hTQLUZ",
        "outputId": "1bb56acf-d9c3-4420-f94e-8d5db87cad31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Effective batch size = 32\n"
          ]
        }
      ],
      "source": [
        "print(f'Effective batch size = {config.batch_size * config.gradient_accumulation_steps}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bKU7Y-6QLUZ"
      },
      "source": [
        "## Tokenizer - OpenAI tiktoken (changed to GPT2Tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvwoPFbiQLUZ",
        "outputId": "6429e3ac-ee37-4ad0-bc36-8dad593f647e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[31373, 995]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#tokenizer = tiktoken.get_encoding(\"cl100k_base\") # gpt4 tokenizer - NOTE: need to change vocab_size in config if used\n",
        "#tokenizer = tiktoken.encoding_for_model('gpt-2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.encode('hello world')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bsUdOVMmjFJ1"
      },
      "outputs": [],
      "source": [
        "tokenizer.model_max_length = config.sequence_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lFDoKw26tevh"
      },
      "outputs": [],
      "source": [
        "# set pad_token_id equal to the eos_token_id if not set\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.eos_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUQERe2CaGmT",
        "outputId": "affd179b-c2f3-45ef-81ff-634b321d59b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ3b6b23QLUb"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FsZa23FJQLUb"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    # def __init__(self, embed_dim, head_size, sequence_length, dropout):\n",
        "    def __init__(self, config, interim_head_size):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.embed_dim\n",
        "        self.interim_head_size = interim_head_size # say embed_dim = 32 -> broken into say 4 heads, so this will be 8, to be concated back to 32\n",
        "        self.key = nn.Linear(config.embed_dim, interim_head_size, bias=config.bias)\n",
        "        self.query = nn.Linear(config.embed_dim, interim_head_size, bias=config.bias)\n",
        "        self.value = nn.Linear(config.embed_dim, interim_head_size, bias=config.bias)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones((config.sequence_length, config.sequence_length))))\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x) # (b,t,c) -> (b,t,h)\n",
        "        q = self.query(x) # (b,t,c) -> (b,t,h)\n",
        "        v = self.value(x) # (b,t,c) -> (b,t,h)\n",
        "        wei = k @ q.transpose(-2, -1) * self.embed_dim**(-0.5) # (b,t,h) @ (b,h,t) -> (b,t,t)\n",
        "\n",
        "        wei = wei.masked_fill((self.tril[:T, :T] == 0.), -torch.inf) # type: ignore\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        xbow = wei @ v # (b,t,t) @ (b,t,h) -> (b,t,h)\n",
        "        return xbow\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    # def __init__(self, num_heads, embed_dim, head_size, sequence_length, dropout):\n",
        "    def __init__(self, config, interim_head_size):\n",
        "        super().__init__()\n",
        "        self.head_list = nn.ModuleList([Head(config, interim_head_size) for _ in range(config.num_heads)])\n",
        "        self.proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.head_list], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(config.embed_dim, 4*config.embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4*config.embed_dim, config.embed_dim),\n",
        "            nn.Dropout(config.dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    # def __init__(self, num_heads, embed_dim, sequence_length, dropout):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.interim_head_size = config.embed_dim // config.num_heads\n",
        "        self.sa = MultiHeadAttention(config, self.interim_head_size)\n",
        "        self.ff = FeedForward(config)\n",
        "        self.ln1 = nn.LayerNorm(config.embed_dim)\n",
        "        self.ln2 = nn.LayerNorm(config.embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) # communication\n",
        "        x = x + self.ff(self.ln2(x)) # computation\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(torch.nn.Module):\n",
        "    # def __init__(self, embed_dim, vocab_size, sequence_length, num_heads, num_blocks, dropout):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.sequence_length = config.sequence_length\n",
        "        self.token_embeddings = torch.nn.Embedding(config.vocab_size, config.embed_dim)\n",
        "        self.position_embeddings = nn.Embedding(config.sequence_length, config.embed_dim)\n",
        "        self.block_list = nn.Sequential(*[Block(config)\n",
        "                                          for _ in range(config.num_blocks)])\n",
        "        self.final_ln = nn.LayerNorm(config.embed_dim)\n",
        "        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size)\n",
        "\n",
        "    def forward(self, ixs, targets=None):\n",
        "        # ixs: (b,t)\n",
        "        # targets: (b,t)\n",
        "        B, T = ixs.shape\n",
        "        x = self.token_embeddings(ixs) # (b,t,c=embed_dim)\n",
        "        pos_embeds = self.position_embeddings(torch.arange(T, device=device)) # (t,c=embed_dim)\n",
        "        x += pos_embeds\n",
        "        x = self.block_list(x)\n",
        "        x = self.final_ln(x)\n",
        "        logits = self.lm_head(x) # (b,t,c=vocab_size)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            mask = (ixs != tokenizer.pad_token_id)  # (b,t), True where not a pad token\n",
        "            logits = logits.permute(0, 2, 1)  # (b,c,t)\n",
        "\n",
        "            # Use the mask to filter out loss on padding positions\n",
        "            # logits are now (b, c, t), targets are (b, t), mask is (b, t)\n",
        "            # Utilizing .masked_fill to turn pad positions to a very large negative value to ignore them in softmax\n",
        "            loss = F.cross_entropy(logits, targets, reduction='none')  # (b, t) get loss per token\n",
        "            loss = (loss * mask).sum() / mask.sum()  # average loss only over non-pad tokens\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, ixs, max_len):\n",
        "        \"\"\"\n",
        "        ixs: (b,t) - input sequence to start generating from\n",
        "        max_len: int - maximum length of the generated sequence\n",
        "        \"\"\"\n",
        "        b, t = ixs.shape\n",
        "        for _ in range(max_len):\n",
        "            # generation (b, ) next tokens in parallel\n",
        "            ixs_cond = ixs[:, -self.sequence_length:] # consider only the last sequence_length tokens\n",
        "            logits, loss = self.forward(ixs_cond) # logits=(b,t,c), loss is ignored\n",
        "            # get juse the final timestep\n",
        "            last_logits = logits[:, -1, :] # (b,c)\n",
        "            # normalize\n",
        "            last_probs = F.softmax(last_logits, dim=-1) # across c\n",
        "            next_tokens = torch.multinomial(last_probs, 1) # (b,c) -> (b)\n",
        "            if (next_tokens == tokenizer.eos_token_id).any():\n",
        "              break\n",
        "            ixs = torch.cat((ixs, next_tokens), dim=1) # across t so (b,t) -> (b, t+1)\n",
        "        return ixs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uM9xrNVCQLUb"
      },
      "outputs": [],
      "source": [
        "# model = Transformer(embed_dim, vocab_size, sequence_length, num_heads, num_blocks, dropout).to(device)\n",
        "model = Transformer(config).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrain_model = Transformer(config).to(device)"
      ],
      "metadata": {
        "id": "wWmwWDXuQFBL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "N8rVNj48QLUc"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "\n",
        "\n",
        "# for generation\n",
        "start_ix = torch.zeros((1,1), dtype=torch.long, device=device) # (newline character in a single batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load models"
      ],
      "metadata": {
        "id": "MSbVcxKQyWRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CKPT_PATH = '/content/hw5/best_fine_tune_model.pth'\n",
        "ckpt = torch.load(CKPT_PATH)\n",
        "model.load_state_dict(ckpt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb92u44if7Xf",
        "outputId": "ada3d4a1-6e60-488f-8b1c-6580c830b9b7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CKPT_PATH = '/content/hw5/best_model.pth'\n",
        "ckpt = torch.load(CKPT_PATH)\n",
        "pretrain_model.load_state_dict(ckpt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWwEFox6QMcs",
        "outputId": "141cdf9a-0ed5-47bd-cca6-c1b9d2ccddae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt, max_seq):\n",
        "    prompt = prompt.strip() # remove leading and ending white spaces - leads to weird things\n",
        "    # Encode the prompt using the tokenizer\n",
        "\n",
        "    # chat_template = f\"<|system|>\\n<|endoftext|>\\n<|user|>\\n{' '.join(prompt.split()[:100])}<|endoftext|>\\n<|assistant|>\"\n",
        "    chat_template = f\"<|user|>\\n{' '.join(prompt.split()[:100])}<|endoftext|>\\n\"\n",
        "    prompt_tokens = tokenizer.encode(chat_template, return_tensors='pt').to(device)\n",
        "\n",
        "    # Generate text using the model\n",
        "    generated_tokens = pretrain_model.generate(prompt_tokens, max_seq)\n",
        "\n",
        "    # Decode the tokens back to text\n",
        "    generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)  # Remove batch dimension\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "uOle6-LMHBpq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics"
      ],
      "metadata": {
        "id": "YhWxYG3rE-Cx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment analysis"
      ],
      "metadata": {
        "id": "XdloKo0--ztQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "judge = pipeline('sentiment-analysis', model=\"finiteautomata/bertweet-base-sentiment-analysis\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNXmC47I7YmN",
        "outputId": "fd2b0fbf-3e83-4545-8d75-4c755160ba23"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "judge('negative')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ8J4gzm40GL",
        "outputId": "743c839a-7829-4301-9186-4651f42476a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEG', 'score': 0.5685986280441284}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z93Lm1fb4qLk",
        "outputId": "cbaeebd2-2c21-4c32-a21b-9dffecddd4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_rows = datasets_test.filter(lambda example: example['Classifier'] == 'SentimentAnalysis')"
      ],
      "metadata": {
        "id": "pB7UnqPmFB5j"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentiment_rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDt6JtNGniRr",
        "outputId": "bdc3d806-947c-4036-d983-de298225e421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "491"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_rows[1]['Messages'][2]['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BaFGdAFh5K4w",
        "outputId": "0c8be773-88fb-48aa-88d7-04925fb31349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'positive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_count = 0\n",
        "nlu_count = 0\n",
        "ii = 0\n",
        "a = '<|assistant|>'\n",
        "for i in sentiment_rows:\n",
        "  target = i['Messages'][2]['content']\n",
        "  q = i['Messages'][1]['content']\n",
        "  if target == 'positive':\n",
        "    target1 = 'POS'\n",
        "  elif target == 'negative':\n",
        "    target1 = 'NEG'\n",
        "  else:\n",
        "    target1 = 'NEU'\n",
        "  new_sentence = generate_text(q, 100)\n",
        "  #generate_sentence = new_sentence.split(a)[-1]#for fine-tuned model\n",
        "  generate_sentence = new_sentence[len(q)+5:len(q)+5+50] # for pre-trained model\n",
        "  print(f'Prompt: {q}')\n",
        "  print(f'Generated sentence: {generate_sentence}')\n",
        "  print(f'Target: {target}')\n",
        "  print('------------')\n",
        "  #print(judge(generate_sentence))\n",
        "  #print(ii)\n",
        "  nlu_gen = judge(generate_sentence)[0]['label']\n",
        "\n",
        "  if target1 == nlu_gen:\n",
        "    nlu_count += 1\n",
        "  if target in generate_sentence:\n",
        "    words_count += 1\n",
        "  ii += 1\n",
        "  if ii == 2:\n",
        "    break\n",
        "acc_with_words = words_count / len(sentiment_rows)\n",
        "acc_with_nlu = nlu_count / len(sentiment_rows)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eekwFGR0ulWP",
        "outputId": "4eee5072-f73e-4ac0-ca4c-a33bf93a8615"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What's the sentiment of the sentence:\n",
            "Clothing retail chain Sepp+ñl+ñ 's sales increased by 8 % to EUR 155.2 mn , and operating profit rose to EUR 31.1 mn from EUR 17.1 mn in 2004 .\n",
            "Generated sentence: .\n",
            "The president told guest Luke Rowley that a pote\n",
            "Target: positive\n",
            "------------\n",
            "Prompt: What's the sentiment of the sentence:\n",
            "HELSINKI ( AFX ) - Shares closed higher , led by Nokia after it announced plans to team up with Sanyo to manufacture 3G handsets , and by Nokian Tyres after its fourth-quarter earnings report beat analysts ' expectations , dealers said .\n",
            "Generated sentence: oming in with a television special this week—and m\n",
            "Target: positive\n",
            "------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to see if there's exactly 'positive','negative' or 'neutral' in generated sentence\n",
        "acc_with_words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlsO9fIkqVlu",
        "outputId": "ff9022a1-2cf3-4545-dd8d-5372ecd94967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4378818737270876"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use Natural Language Understanding tool to analyze the sentiment of generated sentence\n",
        "acc_with_nlu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XUDCme89ZS1",
        "outputId": "3ef8c336-759b-4a07-cddb-f36d0a75f8e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5641547861507128"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to see if there's exactly 'positive','negative' or 'neutral' in generated sentence\n",
        "acc_with_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqfSS7WHRK7x",
        "outputId": "5b039356-3277-41fc-80dc-a0812b70c41b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use Natural Language Understanding tool to analyze the sentiment of generated sentence\n",
        "acc_with_nlu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uXUh2cLRMR-",
        "outputId": "52b0739c-867e-44ef-93c6-933c9befe68a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5254582484725051"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER"
      ],
      "metadata": {
        "id": "xd_3qqUQ-6-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# f-1 score for NER\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "Rc2ypk5AeNBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_rows = datasets_test.filter(lambda example: example['Classifier'] == 'NamedEntity')"
      ],
      "metadata": {
        "id": "oVfebA0QkG6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ner_rows[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZTdHnbkkaRp",
        "outputId": "13e5cdde-c7e9-4393-89ac-240806417775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Classifier': 'NamedEntity', 'Prompt': 'Recognize the named entities from the sentence:\\nSOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .', 'Messages': [{'content': '', 'role': 'system'}, {'content': 'Recognize the named entities from the sentence:\\nSOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .', 'role': 'user'}, {'content': 'JAPAN:B-LOC,CHINA:B-PER', 'role': 'assistant'}], 'PromptId': 'NamedEntity/0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner_rows = ner_rows[:1000]"
      ],
      "metadata": {
        "id": "Fpd53u-JuGak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=ner_rows[0]['Messages'][2]['content']\n",
        "entities = a.split(',')#\n",
        "#keyword, category = entities.split(':')\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duGAddWelZfv",
        "outputId": "65582073-131b-4e61-9f71-0ac8087b5935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['JAPAN:B-LOC', 'CHINA:B-PER']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_key_cat = []\n",
        "predicted_key_cat = []\n",
        "a = '<|assistant|>'\n",
        "ii=0\n",
        "for i in ner_rows:\n",
        "  target = i['Messages'][2]['content']\n",
        "  q = i['Messages'][1]['content']\n",
        "  entities = target.split(',')\n",
        "  true_key_cat.append(entities)\n",
        "  '''\n",
        "  for entity in entities:\n",
        "    if ':' in entity:\n",
        "    #print(ii)\n",
        "    #print(entity)\n",
        "      keyword, category = entity.split(':')\n",
        "      k.append(keyword)\n",
        "      c.append(category)\n",
        "      '''\n",
        "  generate_sentence = generate_text(q, 256).split(a)[-1]#[len(q)+7:]\n",
        "  #generate_sentence = generate_text(q, 90)[len(q)+7:len(q)+7+50]\n",
        "  generated = generate_sentence.split(',')\n",
        "  predicted_key_cat.append(generated)\n",
        "  '''\n",
        "  for gen in generated:\n",
        "    if ':' in gen:\n",
        "      key, cate = entity.split(':')\n",
        "      k1.append(key)\n",
        "      c1.append(cate)\n",
        "      '''\n",
        "  print(f'Prompt: {q}')\n",
        "  print(f'Generated sentence: {generate_sentence}')\n",
        "  print(f'Target: {target}')\n",
        "  print('----------------')\n",
        "  ii += 1\n",
        "  if ii == 500:\n",
        "    break\n",
        "  if ii == 2:\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdmQxsFlj79C",
        "outputId": "96c92d04-d62b-4583-b974-4d980db757e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Recognize the named entities from the sentence:\n",
            "SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .\n",
            "Generated sentence: \n",
            "India:B-LOC,LUCKY:B-LOC,CHINA:B-LOC\n",
            "Target: JAPAN:B-LOC,CHINA:B-PER\n",
            "----------------\n",
            "Prompt: Recognize the named entities from the sentence:\n",
            "Nadim Ladki\n",
            "Generated sentence: \n",
            "Nadim:B-PER\n",
            "Target: Nadim:B-PER,Ladki:I-PER\n",
            "----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit(true_key_cat + predicted_key_cat)\n",
        "true_binary = mlb.transform(true_key_cat)\n",
        "predicted_binary = mlb.transform(predicted_key_cat)"
      ],
      "metadata": {
        "id": "xPZ19PYGlWgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_micro = f1_score(true_binary, predicted_binary, average='micro')\n"
      ],
      "metadata": {
        "id": "BRjKslwE3U64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_micro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vHKvdRy5wJP",
        "outputId": "2210803f-ad14-473f-a190-a4752dadd3bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06988487174308222"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_micro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m80RkOLV5y2c",
        "outputId": "9a7ae31c-6ee3-4ce1-bb3b-89abdb57c593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0009523809523809524"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ri-qaxy_Snlh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}